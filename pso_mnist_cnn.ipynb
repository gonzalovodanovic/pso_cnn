{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización de parámetros de modelos de aprendizaje profundo utilizando Particle Swarm Optimization\n",
    "\n",
    "## Importaciones\n",
    "\n",
    "Lo primeo a realizar es importar todas las bibliotecas que se utilizarán, las más importantes son:\n",
    "\n",
    "- NumPy\n",
    "- TensorFlow \n",
    "- Keras\n",
    "- Optunity\n",
    "\n",
    "Además, se agrega la configuración necesaria para que el experimento sea reproducible. Esta informacion  fue obtenida de la pagina oficial de Keras (https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development). Para que el resultado sea exactamente el mismo en todas las ejecuciones no es posible utilizar paralelismo, esto hace que el tiempo de ejecución aumente considerablemente. Sin embargo, la diferencia entre experimentos al utilizar paralelismo es baja (tercer digito) por lo que se decidió aceptar esta diferencia y reducir los tiempos de computo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(1234)\n",
    "import random as rn\n",
    "rn.seed(12345)\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0' \n",
    "from keras import backend as K\n",
    "\n",
    "import math\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import optunity\n",
    "import optunity.metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer, Input\n",
    "from keras.layers import Reshape, MaxPooling2D\n",
    "from keras.layers import Conv2D, Dense, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Funciones adicionales para graficar (blackbox)\n",
    "def plot_example_errors():\n",
    "    # Use TensorFlow to get a list of boolean values\n",
    "    # whether each test-image has been correctly classified,\n",
    "    # and a list for the predicted class of each image.\n",
    "    correct, cls_pred = session.run([correct_prediction, y_pred_cls],\n",
    "                                    feed_dict=feed_dict_test)\n",
    "\n",
    "    # Negate the boolean array.\n",
    "    incorrect = (correct == False)\n",
    "    \n",
    "    # Get the images from the test-set that have been\n",
    "    # incorrectly classified.\n",
    "    images = data.test.images[incorrect]\n",
    "    \n",
    "    # Get the predicted classes for those images.\n",
    "    cls_pred = cls_pred[incorrect]\n",
    "\n",
    "    # Get the true classes for those images.\n",
    "    cls_true = data.test.cls[incorrect]\n",
    "    \n",
    "    # Plot the first 9 images.\n",
    "    plot_images(images=images[0:9],\n",
    "                cls_true=cls_true[0:9],\n",
    "                cls_pred=cls_pred[0:9])\n",
    "\n",
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()\n",
    "\n",
    "def print_accuracy():   \n",
    "    # Use TensorFlow to compute the accuracy.\n",
    "    acc = session.run(accuracy, feed_dict=feed_dict_test)\n",
    "    \n",
    "    # Print the accuracy.\n",
    "    print(\"Accuracy on test-set: {0:.1%}\".format(acc))\n",
    "    \n",
    "    \n",
    "def print_confusion_matrix():\n",
    "    # Get the true classifications for the test-set.\n",
    "    cls_true = data.test.cls\n",
    "    \n",
    "    # Get the predicted classifications for the test-set.\n",
    "    cls_pred = session.run(y_pred_cls, feed_dict=feed_dict_test)\n",
    "\n",
    "    # Get the confusion matrix using sklearn.\n",
    "    cm = confusion_matrix(y_true=cls_true,\n",
    "                          y_pred=cls_pred)\n",
    "\n",
    "    # Print the confusion matrix as text.\n",
    "    print(cm)\n",
    "\n",
    "    # Plot the confusion matrix as an image.\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "\n",
    "    # Make various adjustments to the plot.\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    plt.xticks(tick_marks, range(num_classes))\n",
    "    plt.yticks(tick_marks, range(num_classes))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()\n",
    "\n",
    "def plot_weights():\n",
    "    # Get the values for the weights from the TensorFlow variable.\n",
    "    w = session.run(weights)\n",
    "    \n",
    "    # Get the lowest and highest values for the weights.\n",
    "    # This is used to correct the colour intensity across\n",
    "    # the images so they can be compared with each other.\n",
    "    w_min = np.min(w)\n",
    "    w_max = np.max(w)\n",
    "\n",
    "    # Create figure with 3x4 sub-plots,\n",
    "    # where the last 2 sub-plots are unused.\n",
    "    fig, axes = plt.subplots(3, 4)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Only use the weights for the first 10 sub-plots.\n",
    "        if i<10:\n",
    "            # Get the weights for the i'th digit and reshape it.\n",
    "            # Note that w.shape == (img_size_flat, 10)\n",
    "            image = w[:, i].reshape(img_shape)\n",
    "\n",
    "            # Set the label for the sub-plot.\n",
    "            ax.set_xlabel(\"Weights: {0}\".format(i))\n",
    "\n",
    "            # Plot the image.\n",
    "            ax.imshow(image, vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "\n",
    "        # Remove ticks from each sub-plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset\n",
    "\n",
    "Se utiliza el conjunto de datos MNIST, el cual contiene 70.000 imagenes de 28x28 pixeles y las etiquetas verdaderas correspondientes. Cada una de las imagenes contiene un número del 0 al 9 escritos a mano, la etiqueta es un vector de 10 elementos con un uno en la ubicación (índice) que se corresponde con el valor representado en la imagen y los demas elementos en cero (codificacion \"one-hot\").\n",
    "\n",
    "Este conjunto de datos se divide en tres subconjuntos:\n",
    "\n",
    "- Entrenamiento\n",
    "- Test\n",
    "- Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/t10k-labels-idx1-ubyte.gz\n",
      "Size of:\n",
      "- Training-set:\t\t55000\n",
      "- Test-set:\t\t10000\n",
      "- Validation-set:\t5000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets('data/MNIST/', one_hot=True)\n",
    "#data.test.cls = np.array([label.argmax() for label in data.test.labels])\n",
    "data.test.cls = np.argmax(data.test.labels, axis=1)\n",
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(data.train.labels)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(data.test.labels)))\n",
    "print(\"- Validation-set:\\t{}\".format(len(data.validation.labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"inData\"></a> Dimnsiones de los datos \n",
    "Se declaran variables que serán utilizadas a lo largo del código y que definen las dimensiones de los datos. Además, se muestran algunas imágenes del set de datos que se esta utilizando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUMAAAD5CAYAAAC9FVegAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHilJREFUeJzt3XmUFNXZx/HvA0KQTUVQUHHmBFwgRFExuGsUiCICEheM\nCzFGIxrcEjAaF1xilKBwRE/YjoQTNCgKiEYEQUV8EZAIiowbiCgQlxHigogI9/1j5nZVz/TsXVU9\n7e9zjmequ6qrnvHSd566dRdzziEi8kPXIOkARERygSpDERFUGYqIAKoMRUQAVYYiIoAqQxERQJWh\niAigylBEBFBlKCICwC41Obh169ausLAwolByzwcffEBxcbElHUecVMb5T2WcWY0qw8LCQpYtW1b7\nqOqZbt26JR1C7FTG+U9lnJluk0VEUGUoIgKoMhQRAVQZiogAqgxFRIAaPk0Wqa2RI0cCsHXrVgDe\neOMNAB5//PFyxw4ePBiAo48+GoALL7wwjhDlB06ZoYgIygwlYueeey4A06ZNy7jfrHxf2LFjxwIw\nb948AE488UQA9t9//yhClAS9++67ABx00EEA3H///QAMGTIk9liUGYqIoMxQIuCzQag4Izz44IMB\nOPXUUwF4//33U/tmzZoFwOrVqwGYMmUKADfeeGP2g5VELV++HIAGDUrysn333TexWJQZioigzFCy\nyI93nTFjRrl9Xbp0AYKsr3Xr1gA0b94cgO+++y51bPfu3QF4/fXXAfj8888jiliStmLFCiD4dzBg\nwIDEYlFmKCJCDJmh70c2YcIEAPbZZ5/UviZNmgBw/vnnA9C2bVsAOnbsGHVYEoH//ve/ADjnUu/5\njHDOnDkAtGvXLuNnfT9EgLfeeittX58+fbIapyRv5cqVAIwZMwaAiy66KMlwAGWGIiJADJnh0KFD\ngZIJFivi+5W1bNkSgM6dO2fl2u3btwdg2LBhwA9z7ro4nXHGGUDwFBigRYsWALRq1arSzz766KOp\n7XD7oeSnd955B4AtW7YA6T0QkqLMUEQEVYYiIkAMt8kTJ04Egm4S4VvgoqIiIOh4+eKLLwKwePFi\nIBh+9eGHH1Z4/kaNGgFBVw3fiB8+j79d1m1yPAoKCqp97N/+9jcgGJYV5rvY+J+SP0aMGAGULEEA\nufHdVGYoIkIMmeEpp5yS9jPMD8XyNm/eDASZov9r8eqrr1Z4/h/96EdAMNDbD/MC2LRpEwAdOnSo\nVewSnaeffhqAW265BYBt27al9u29994A3H333QA0bdo05ugkCuGHqP477b+3zZo1SyKkNMoMRUTI\nseF4e+yxBwAnn3xy2vuZssqynnjiCSDILgEOOeQQAAYOHJitECVL/NC9cEbo+W4WfuouyQ8LFiwo\n916bNm0SiCQzZYYiIuRYZlgbn376KQBXXHEFkD4UzLdHVdXhV+LTv39/IBie5w0aNCi1feedd8Ya\nk8TDL/UQ5gdE5AJlhiIi5EFm+OCDDwJBhrj77run9vknVZI83/9z0aJFQNBW6NuMbrrpptSxfjon\nyQ+vvPIKAJMmTUq9d9hhhwHQs2fPRGLKRJmhiAj1ODN8+eWXgaAvmvfkk0+mtv30UZI8P2lncXFx\n2vt++jb1Bc1f8+fPB9J7evg+xn4av1ygzFBEBFWGIiJAPb5NfuaZZ4Bg7rsePXoAcPTRRycWk5Tn\n1zzxQyy9k046CYDbb7897pAkZn6SlrCzzz47gUgqp8xQRIR6mBlu3boVgGeffRYIJmq47bbbgGBK\nL0lOeDW7u+66Cyg/e3XXrl0BdaPJZx9//DEACxcuBNInUTnzzDMTiakyygxFRKiHmaGfDNS3QZ12\n2mkAHHPMMYnFJOnuvffe1PbSpUvT9vnheGorzH//+Mc/APjkk0+A4Luaq5QZiohQTzJDPxEowB13\n3AHAbrvtBsDNN9+cSExSsfvuu6/CfX74pNoK89+6devSXvsp+nKVMkMREXI8M/RPJa+66qrUe99/\n/z0AvXv3BtSvsL7xZVqdp/4++/fHbt++HYAvvvii3LF+qNeoUaMynqthw4ap7XvuuQfQcgJRe+qp\np9Je9+nTJ6FIqkeZoYgIqgxFRIAcvU3esWMHEMxssXbt2tS+jh07AsGDFKlf/Lo01XHOOecA0K5d\nOyDoojF16tQ6xeBX3wvPoSjZ4ztZ+/KqL5QZioiQo5nhmjVrgGAFtTDfbUPz3+Uu/3ALYObMmbU+\nz2OPPVblMf7hSoMG6X/X+/btCwRrb4cdd9xxtY5JqjZjxgwgeNjpZ7XO9dUOlRmKiJBjmaHvpNmr\nV6+090eOHJnazvXH8wLTp09PbY8YMQIoP1GDV1RUBFTeDnjJJZcAUFBQUG7fL3/5SwA6depUu2Al\na7755hsAZs+enfa+n64r3L0pFykzFBEhxzLDcePGAeWH8YTbGsws1pikbqq7Lu4jjzwScSQSNd9+\n61eo7NevHwBXX311YjHVhDJDERFyJDP0/ZIeeOCBhCMRkdrymaFfJ7m+UWYoIkKOZIZ+DeSvvvoq\n7X0/2kTTPYlI1JQZioigylBEBMiR2+Sy/Mpp8+fPB6BVq1ZJhiMiPwDKDEVEyJHM8IYbbkj7KSIS\nN2WGIiKAOeeqf7DZZ8C6Kg/MHwXOuTZJBxEnlXH+UxlnVqPKUEQkX+k2WUQEVYYiIkDET5PNbE9g\nfunLtsAO4LPS1z9zzmWe8bNu1+wMhOeD6gDc4JzTLBARSKiMC4DJwF6AA/6u8o1OEmVcet3JQG9g\ng3OuaxTXSLteXG2GZjYc+No5N7LM+1Yax84IrtkI2AAc7pxbn+3zS7q4ytjM9gH2cs6tMLOWwHLg\nNOfcu9k4v1Qszu+xmZ0IbAXGx1EZJnKbbGYdzazIzB4GVgHtzex/of0DzWxi6fbeZjbdzJaZ2VIz\nO6oGl+oJvKWKMH5RlrFzbqNzbkXp9pfA28C+0f02kknU32Pn3AJgU2S/QBlJthkeDIxyznWmJHur\nyP3ACOdcN+AcwP/P7W5mY6u4xkDgX9kIVmol8jI2sx8DXYBXsxOy1FAc3+NYJDkCZY1zrvxaoOX1\nAA4KTfe/h5nt6pxbAiyp6ENm1gQ4HbiuzpFKbUVdxi2BJ4Ahzrmv6xyt1EakZRynJCvDLaHtnUB4\ncZMmoW2jdo20pwNLnHPFtYxP6i6yMjazxsB0YJJzbladopS6iPp7HJuc6FpT2ui62cwOMLMGwJmh\n3fOAK/0LM6tuQ+p56BY5Z2SzjEsb6/8BrHDO3R9BuFILEX2PY5MTlWGp64E5wCIg/MDjSuBYM3vD\nzIqAS6HytgYzawH8HJgZbchSQ9kq4xMp+WPX08xWlP73i4hjl+rJ5vd4GrAQ6Gxm683s11EGruF4\nIiLkVmYoIpIYVYYiIqgyFBEBVBmKiACqDEVEgBp2um7durUrLCyMKJTc88EHH1BcXGxVH5k/VMb5\nT2WcWY0qw8LCQpYtq87Im/zQrVu3pEOInco4/6mMM9NtsogIqgxFRABVhiIigCpDERFAlaGICKDK\nUEQESHZy1wpt2VIyX+TQoUMBGDs2mOHHPyafNm0aAAUFBTFHJyL5SJmhiAg5mhlu3LgRgAkTJgDQ\nsGHD1D7fWfSpp54C4Pe//33M0UltvPbaawAMGDAAKBkVUFtz585NbXfq1AmA9u3b1z44SYz/Hvft\n2xeAMWPGADB48ODUMeHvf5SUGYqIkGOZ4WeffQbAoEGDEo5Esm3OnDkAbNu2rc7nmjUrWP/poYce\nAmDq1Kl1Pq/E5/PPPwfSM0CAIUOGAHDJJZek3tt1111jiUmZoYgIOZIZ3n9/yQJnM2eWrN/06qtV\nrwe+cOFCAPwaLoceeigAJ5xwQhQhSi19//33ADzzzDNZO2d44P19990HBD0QmjVrlrXrSHReeukl\nADZsSF93/rzzzgOgSZMm5T4TNWWGIiLkSGZ4zTXXADV7ajR9+vS0n/vvvz8Ajz32WOqYI444Ilsh\nSi298MILACxatAiA66+/vs7n3LRpU2p71apVAHzzzTeAMsNcFm4vvvPOOzMec+GFFwJQsjR2vJQZ\nioigylBEBEj4Nrl3795A8BBkx44dVX6mdevWQHA7tG7dOgDWrl0LwJFHHpk6dufOndkLVqpt5cqV\nqe2BAwcC0LFjRwBuvPHGOp8/3LVG6o833ngjte074Xu77FJSFZ122mmxxhSmzFBEhAQywwULFqS2\n3377bSBoLK3oAcrll1+e2u7VqxcAu+22GwDPP/88AH/5y1/Kfe7vf/87UL5jp0QrXBb+wcaUKVMA\naN68ea3P6x+chP8NJdHQLrXjH3Zm0rNnzxgjyUyZoYgIMWaGfmC+b0MCKC4uznis7yZz1llnAXDr\nrbem9jVt2jTtWD+F17hx48qdc9iwYQB8++23QDCpQ6NGjWr3S0ilHn/8cSC9g7VvKwy35daW744R\nzgZPOukkAHbfffc6n1+iFc7ovcaNGwNw1113xR1OOcoMRUSIMTPcvn07UHE2CMFQukcffRQInhxX\nxmeG/inlddddl9rnh2j5DNFPE9ShQ4caxS7V4yfc9f/fITvttf6u4pFHHgGCJ48AN910E6BsP5f5\nDvevvPJKuX3+Tq9r166xxpSJMkMREXJkOJ5vT5o0aRJQvYywLJ/1Pfzww6n3li5dmoXopCpffPEF\nAIsXLy6374orrqjz+cePHw8EU7x17tw5te/kk0+u8/klWpVNvJJLPT2UGYqIkEBmmGmUyZIlS+p8\nXj+KJTzqpOzIFv9U2vd5k+zwA/DXr18PBNMwZcuaNWvSXnfp0iWr55doZcoM/dP/bNw5ZIsyQxER\nVBmKiAAx3ib7tY+jWunKr7K1fPny1Htlh/nddtttkVz7h65FixZA0D0iPFGDH0LXqlWrGp/3008/\nBYIuO96xxx5bqzglXi+//DIQdIkK88Np99tvv1hjqowyQxERYswMn3766ayez3ezKCoqAiofzuO7\n6qhjbjT86mV+6J0flgdw+umnA+md4TN58803U9v+gYmfnq3sZAwNGuhveH3gV8DzDzLDcmFihrL0\nr0pEhBzpdF0bfpqoBx98sMJjCgsLAZg8eTIQTAAh0Rg+fDiQngn4O4LwBB2ZtGnTJrXtM8GKhm5e\nfPHFdQlTYlK2rTc8mcZll10WdzhVUmYoIkI9zAz9UgF+YtjK+GFbxx9/fKQxSYlOnToB6SsU+qf7\nZTtOl+WnawsbNGgQUL6TvG+jlNzkO9+XfYocfnKcjSndsk2ZoYgIMWaGlS36NHv27LTXl156KQAb\nN26s8DzVme4920+wpeYOO+ywtJ818eMf/zjj++F+jD/96U9rF5hExk/ZVfYpcr9+/ZIIp9qUGYqI\noMpQRASI8TbZz1vmZ50O8x1zyw7VyzR0z99mV2clPanf/G1W2dst3RrnNt/Z2vODHq655pokwqk2\nZYYiIsSYGQ4YMACAESNGpN6rbD2Uqvi/Nr47x4QJEwBo165drc8pucU/JNPayPXLnDlz0l63b98e\nCCZnyFXKDEVEiDEz9KvY+ZXvAGbOnAnA6NGja3y+P//5z0CwFrLkH7/etafO1rnNr4C5evXqtPeb\nNGkC5P5EKcoMRURIYDieXxs5vN2rVy8gWAXNT9R6xhlnAPC73/0u9Rn/ZDG8QprkJ79aoh/gf8st\ntyQZjlTBT63mh9qtWrUKgAMOOCCxmGpCmaGICDkyUcOpp56a9lMEggzj2muvBbRGcq7zfX/99Hq+\nF8Dhhx+eWEw1ocxQRIQcyQxFMvFtx1K/7LPPPgA89NBDCUdSM8oMRURQZSgiAqgyFBEBVBmKiACq\nDEVEAFWGIiIAWKbV7is82OwzYF104eScAudcm6oPyx8q4/ynMs6sRpWhiEi+0m2yiAiqDEVEAFWG\nIiJAxGOTzWxPYH7py7bADuCz0tc/c859F9F1ewOjgIbAOOfc36K4jiRXxqXX3gV4DXjfOdc/quv8\n0CX4PZ4M9AY2OOe6RnGNtOvF9QDFzIYDXzvnRpZ530rj2Jml6zQC3gF+DnwMLAN+6Zx7Nxvnl4rF\nVcah8w4DugJNVRnGI84yNrMTga3A+Dgqw0Ruk82so5kVmdnDwCqgvZn9L7R/oJlNLN3e28ymm9ky\nM1tqZkdVcfqjgLecc+ucc9uAx4B+Uf0uklnEZYyZFQA9gUlR/Q5SuajL2Dm3ANgU2S9QRpJthgcD\no5xznYENlRx3PzDCOdcNOAfw/3O7m9nYDMfvC3wUer2+9D2JX1RlDDAaGAqob1iyoizjWCU5n+Ea\n59yyahzXAzgotHbuHma2q3NuCbAksugkGyIpYzPrD3zknFthZj2yF67UQt58j5OsDLeEtncC4ZXC\nm4S2jZo10m4A2ode70flf7EkOlGV8THAADPrW3qelmY22Tk3qE7RSm1EVcaxy4muNaWNrpvN7AAz\nawCcGdo9D7jSvzCzqhpSFwOdzazAzH5ESUo+K9sxS81ks4ydc8Occ/s55wqBC4C5qgiTl+Xvcexy\nojIsdT0wB1hESTufdyVwrJm9YWZFwKVQcVuDc247cBXwHFAETHHOvRN18FItWSljyWlZK2MzmwYs\npCS5WW9mv44ycI1NFhEhtzJDEZHEqDIUEUGVoYgIoMpQRASoYT/D1q1bu8LCwohCyT0ffPABxcXF\nVvWR+UNlnP9UxpnVqDIsLCxk2bLqdDbPD926dUs6hNipjPOfyjgz3SaLiKDKUEQEUGUoIgKoMhQR\nAVQZiogAqgxFRABVhiIiQLKTu4qIALB582YAPvzwwwqPKSgoAGDUqFEAdOnSBYADDzwQgEMPPbRO\nMSgzFBEh4czw008/BeCcc84B4JhjjgHgsssuA0p6ymfDF198AcBLL70EwKmnngpAo0aNsnJ+EamZ\np59+GoCnnnoKgBdffBGA9957r8LPHHTQQUDJ8DqAbdu2pe3fubNuq5QqMxQRIYHM0LcNAPzkJz8B\ngsxt7733BrKfER5++OEAFBcXA6TGZR5wwAFZuY5U35dffgnAn/70JwBWrVoFwLx581LHKGPPD2vW\nrAHgwQcfBGD8+PGpfVu3bgWgJjPtv/NOtKt3KDMUESHGzNBnZb59EODzzz8H4MorSxbNGjNmTFav\neeeddwKwdu1aIPjLpIwwflOmTAHgpptuAso/NfQZI8Cee+4ZX2ASmfXrS9aDGj16dJ3Oc/DBBwPB\n0+OoKDMUESHGzPC1114DgqdGYbfcckvWrvPmm2+mtkeOHAnAmWeWLN967rnnZu06Uj0+O7j22muB\n4A7BLH2uzSFDhqS2H3jgAQBatWoVR4hSC74cIcj8jjvuOCDordG4cWMAdtttNwCaN2+e+szXX38N\nwC9+8QsgyPq6d+8OwGGHHZY6dtdddwWgWbNmWf4t0ikzFBFBlaGICBDDbbLvWP3EE0+U2/fQQw8B\n0KZNmzpfx98e9+zZs9y+AQMGANCiRYs6X0dqxjdV+IdlFZk6dWpqe/bs2UDwsMXfQvvbLknOli1b\ngPTv2euvvw7AzJkz0449+uijAVi+fDmQ3mXOP0Dbb7/9AGjQIPm8LPkIRERyQOSZ4R/+8Acg6Frh\nO0ADnH322Vm7zssvvwzAxx9/nHrv4osvBuCCCy7I2nWkauvWrUttT5o0KW2fH0zvO9g/99xz5T7v\nO8v7rPL8888HoG3bttkPVqrlu+++A+BXv/oVEGSDADfeeCMAPXr0yPjZTIMo9t9//yxHWHfKDEVE\niCEz9F0o/M999903ta8ubUB+OM9dd90FBEN+wl02fJukxGvFihWpbd+Z+oQTTgBgwYIFAHz77bcA\nPPLIIwD89a9/TX1m9erVQJDl9+vXDwjaEtXlJj6+C4z/nvmJFcLt/EOHDgWgadOmMUeXXcoMRURI\nYKIGP3UPQK9evQDYfffdARg8eHCVn/edtv3PxYsXp+3PZjuk1E54aiWfqftO116TJk0A+M1vfgPA\n448/ntrnB/j7Qfw+49DT5Pj5J8R33303EEywunDhwtQxvlN1fafMUESEGDLDq6++GoDnn38egI0b\nN6b2+fYjnwE8+eSTVZ7PH1t2OFeHDh2AoG1DkvOvf/2r3Hv//ve/Aejfv3/Gz/hp1TI56qijgPTh\nXBKPRYsWpb32w+R8/8B8osxQRIQYMsMjjjgCgJUrVwLpTxqfffZZAEaMGAHAXnvtBcCgQYMqPN+F\nF14IwCGHHJL2vl8ywGeIkpzzzjsvte2z/VdffRWAt99+Gwj+PcyYMQNIn/TXtyH79/zUa77sO3fu\nHFnski7clgvBE/3bbrst9V7fvn2B9MkV6iNlhiIiqDIUEQHAarIGQbdu3VxlDd1xeP/994Hgdrhr\n164AzJ07F8jOpA9et27dWLZsmVV9ZP7IRhlv2rQpte3LyQ+xq+gBWHjgv+9A36dPHwDeffddIFg1\ncezYsXWKL0xlXLmygyYyadiwIQCXX345EMxJ+NFHHwHQsWNHIFjzKMyvgeMndYjiwUx1y1iZoYgI\nCa+bXBu33347EPyl8g9fspkRSt2Eh8tNmzYNgLPOOgsonyFeddVVANxzzz2pz/gO2X7qNT9Ub86c\nOUDQKRv0wCxqf/zjHwG49957Kzxmx44dQJDR+5814R+ennTSSUD6lG5xUWYoIkI9yQx9dgEwefJk\nAFq2bAloJbVc56d18l00/MQMvvuMz/R9Nhh28803A/DWW28BQTcd/xkI/j1INPwwPL+qpZ9Obfv2\n7alj/Do3PkOsDT8JtP+uh1fC85P8Rk2ZoYgI9SQz9B09w04//XQgfbJYyV0+Q6xoAtBM/KpoflVD\nnxm+8MILqWP8k2tN6xUN/6T4yCOPBIIn+2Hz588Hgmxx+PDhACxdurTG1/Ntyf/5z39q/Nm6UmYo\nIkI9zAz92qn+KZfkP99eNWvWLCD9SaNfYzmba29LzZxyyilpr/2QW58ZNmrUCAiW4QC49NJLARg1\nahQQtCUnSZmhiAiqDEVEgBy/TfbDrsIr3vlV1fTg5IfDr6k7bNgwIH19Xt9YP3DgQAAOPPDAeIOT\ncvwM9n7VPP9gxc8+BPDee+8BwYz1ZYXXSoqLMkMREepJZhgeJN67d++0Y7766isgmPsuF9djlezw\nk3Lccccdqff8g7QbbrgBCNbn9t1yJH6dOnUCgi5Rjz76aLljwt2jAHbZpaQq8l3mwsMz46LMUESE\nHM8MM/F/QXwG4B/N++E7Gp6V/y666KLU9rhx4wCYPn06ELRFlZ0JXeLjs/LRo0cDwd1buCP1J598\nAkBhYSEQlKlvA06CMkMREephZjhhwgQAJk6cCMBvf/tbIBjUL/kvPF3bvHnzgGA9Xz+xQC504v2h\n8z0//Frp//znP1P7XnnlFSDIBP0UXklSZigiQo5nhmPGjAHg1ltvTb13wgknADB48GAA9thjDwAa\nN24cc3SSC3zvAb9sgB+yV1RUBGglvVziVzcsu50rlBmKiJDjmeHxxx8PwPPPP59wJJLr/OSxhx56\nKACrV68GlBlK9SkzFBFBlaGICJDjt8ki1eXXxFm7dm3CkUh9pcxQRARVhiIigCpDEREAzK9GVa2D\nzT4D1kUXTs4pcM61qfqw/KEyzn8q48xqVBmKiOQr3SaLiKDKUEQEiLifoZntCcwvfdkW2AF8Vvr6\nZ8657yK89i7Aa8D7zrn+UV3nhy6pMjaz64BLSl+Odc6NieI6kmgZrwc2l15vm3OuexTXSV0vrjZD\nMxsOfO2cG1nmfSuNY2eWrzcM6Ao0VWUYj7jK2My6ApOBo4DvgbnAb5xz6nEdsTi/x6WVYRfn3P+y\ndc7KJHKbbGYdzazIzB4GVgHtzex/of0DzWxi6fbeZjbdzJaZ2VIzO6oa5y8AegKTovodpHIRl3En\nYLFzbqtzbjvwEnBmVL+LZBb19zhuSbYZHgyMcs51BjZUctz9wAjnXDfgHMD/z+1uZmMr+MxoYCig\nR+XJiqqMVwInmlkrM2sGnAa0z27oUk1Rfo8d8KKZ/cfMLqngmKxJcmzyGufcsmoc1wM4KLRc6B5m\ntqtzbgmwpOzBZtYf+Mg5t8LMemQvXKmFSMrYOfemmd0HzAO+BpZT0q4k8YukjEsd5ZzbYGZtgefM\n7C3n3KIsxJxRkpXhltD2TsBCr5uEto2aNdIeAwwws76l52lpZpOdc4PqFK3URlRljHNuPDAewMxG\nAKvrEKfUXpRlvKH058dm9iTwMyCyyjAnutaUNrpuNrMDzKwB6e0/84Ar/YvSxvPKzjXMObefc64Q\nuACYq4owedks49Jj9ir9WQj0BaZmM16puWyWsZk1N7PmpdvNKHkG8Gb2ow7kRGVY6npgDiU1//rQ\n+1cCx5rZG2ZWBFwKVbY1SG7KZhnPLD12JnC5c+7LCOOW6stWGbcD/s/MXgeWAjOcc/OiDFzD8URE\nyK3MUEQkMaoMRURQZSgiAqgyFBEBVBmKiACqDEVEAFWGIiKAKkMREQD+H2ExW84Ko5cxAAAAAElF\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c341985f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We know that MNIST images are 28 pixels in each dimension.\n",
    "img_size = 28\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = img_size * img_size\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "# This is used for plotting the images.\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Tuple with height, width and depth used to reshape arrays.\n",
    "# This is used for reshaping in Keras.\n",
    "img_shape_full = (img_size, img_size, 1)\n",
    "\n",
    "# Number of colour channels for the images: 1 channel for gray-scale.\n",
    "num_channels = 1\n",
    "\n",
    "# Number of classes, one class for each of 10 digits.\n",
    "num_classes = 10\n",
    "\n",
    "# Get the first images from the test-set.\n",
    "images = data.test.images[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = data.test.cls[0:9]\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "\n",
    "El propósito de TensorFlow es crear grafos computacionales (_computational graph_) que pueden ser ejecutado mas eficientemente que si el cálculo fuera realizado directamente en _Python_. TensorFlow puede ser mas eficiente que NumPy porque conoce el grafo computacional completo, mientras que NumPy únicamente conoce de una operación matemática por vez.\n",
    "\n",
    "Además, TensorFlow puede calcular autmáticamente los gradientes necesarios para optimizar las variables de los grafos y mejorar la performance del modelo. Esto se debe a que los grafos son una combinación de expresiones matemáticas simples, por lo que el gradiente del grafo se obtiene mediante la _chain rule_ de las derivadas.\n",
    "\n",
    "Un grafo de TensorFlow consiste de las siguientes partes:\n",
    "\n",
    "- Variables _placeholder_: usadas para la entrada de datos al grafo.\n",
    "- Variables del modelo: son aquellas que se modifican durante la optimización del moderlo.\n",
    "- Modelo: es esencialmente una funcion matemática que calcula una salida a partir de los datos entrada (las variables _placeholder_ y las variables del modelo).\n",
    "- Funcion de costo: se utiliza para guiar la optimización de las variables.\n",
    "- Método de optimización: actualiza las variables del modelo.\n",
    "\n",
    "## Variables _Placeholder_\n",
    "\n",
    "Las variables _placeholder_ sirven de entrada al grafo y se modifican cada vez que se ejecuta el mismo. En este caso de ejemplo, las variables _placeholder_ son las imágenes de entrada tratadas como tensores. El tipo de datos se fija a _float32_ y la forma se configura como: $ [None, img\\_size\\_flat] $ donde _None_ significa que el tensor puede almacenar un número arbitrario de imagenes donde cada una es un vector de tamaño $img\\_size\\_flat$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, img_size_flat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo siguiente es crear una variable _placeholder_ donde se asocia las imagenes de entradas con las etiquetas verdaderas. La forma de esta variable es $[None, num\\_clases]$ lo que implica que puede almacenar un número arbitrario de etiquetas y cada una de ellas es un vector del largo $num\\_clases$, en este caso es 10.\n",
    "Finalmente se crea otra variable _placeholder_ similar, donde la clase real de cada imagen esta en formato _integer_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(tf.float32, [None, num_classes])\n",
    "y_true_cls = tf.placeholder(tf.int64, [None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables a optimizar del modelo\n",
    "\n",
    "Además de las variables de _placeholder_, se deben definir variables que se utilizarán para modificar y optimizar el modelo a partir de los datos de entrenamiento. Estas variables, llamadas _weights_ (pesos) y _biases_ (sesgo), se explicarán en detalle en la siguiente sección. \n",
    "\n",
    "La forma de la variable de pesos es $[img\\_size\\_flat, num\\_classes]$, es decir, es un tensor de dos dimensiones con $img\\_size\\_flat$ filas y $num\\_classes$ columnas. La variable llamada _bias_ es un tensor de una dimensión y de largo $num\\_classes$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.zeros([img_size_flat, num_classes]))\n",
    "biases = tf.Variable(tf.zeros([num_classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo: función de calidad (_Score function_)\n",
    "\n",
    "Esta es una función que indica la calidad de la clasificación de un dato determinado para cada una de las categorias disponibles. Si se asume que el conjunto de entrenamiento contiene $x_i \\in R^D$ cada uno asociado a una etiqueta $y_i$, siendo $i = 1 \\, ... \\, N$ y que $y_i \\in 1 \\, ... \\, K$. Entonces, se tienen $N$ ejemplos de dimensión $D$ y $K$ categorias. \n",
    "Por ejemplo, en el conjunto de datos de entrenamiento de MNIST se tienen $N = 55000$ imágenes, cada una de $D = 28 x 28 = 784$ pixeles. Debido a que existen 10 categorias (0 al 9), $K=10$. \n",
    "\n",
    "Se define a la función \"score\" como:\n",
    "\\begin{equation}\n",
    "f : R^D \\mapsto R^K\n",
    "\\end{equation}\n",
    "donde se mapean los datos de entrada a los \"score\" de cada clase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificador lineal\n",
    "\n",
    "El clasificador lineal es un modelo simple donde se multiplican las imágenes en las variables _placeholder_ con los pesos (_weight_) y se les suma el sesgo (_bias_):\n",
    "\\begin{equation}\n",
    "s = f(x_i, W,b) = Wx_i \\, + \\, b\n",
    "\\end{equation}\n",
    "\n",
    "En el procesamiento de imagenes, $x_i$ contiene todos los pixeles de una imagen dada en la forma de vector columna $[D \\, \\times \\, 1]$. La matriz $W$ de tamaño $[K \\, \\times \\, D]$ son los pesos de la función y el vector de bias $b$ de tamaño $[K \\, \\times \\, 1]$ se denomina asi dado que influye en el resultado final pero sin interactuar con las entradas $x_i$. En conjunto, conforman los parámetros de la función y generalmente se combinan en una unica matrix de tamaño $[K \\, \\times \\, (D + 1) ]$, esto hace necesario modificar el vector columna $X_i$ de la misma manera agregando la constante 1. \n",
    " \n",
    "\\begin{equation}\n",
    "s = f(x_i, W) = Wx_i\n",
    "\\end{equation}\n",
    "\n",
    "Volviendo al ejemplo del conjunto de datos MNIST, $x_i$ es un vector columna de la forma $[785 \\, \\times \\, 1]$ y la matriz de parámetros conformada por $W$ en conjunto con $b$ tiene la forma $[10 \\, \\times \\, 785]$. Utilizando la nomenclatura de la sección \"Dimensiones de los datos\" [link](#inData) la forma de las imágenes de entrada es $ x = [num\\_images, img\\_size\\_flat ]$ y la de los pesos $W = [num\\_images, num\\_classes]$ por lo que el resultado tiene la forma $[num\\_images, num\\_classes]$ luego se suma el vector de sesgos a cada fila de la matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.matmul(x, weights) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_logits_ es una matriz de $num\\_images$ filas y $num\\_classes$ columnas, donde el elemento de la fila $i$ y de la columna $j$ es un estimado de la probabilidad de que la imagen de entrada $i$ pertenezca a la clase $j$. \n",
    "Nota: el nombre logits es tipico de la terminologia de _TensorFlow_.\n",
    "\n",
    "Se puede ver que a partir de la multiplicación de matrices $W \\, x_i$ efectivamente se estan evaluando 10 clasificadores en paralelo (uno por clase) donde cada clasificador es una fila de $W$. Además, aunque $(x_i,y_i)$ son fijos, se pueden modificar los parámetros $W,b$ hasta obtener valores de \"score\" que coincidan con la tabla de verdad durante el entrenamiento. Es decir, los parametros $(W,b)$ se aprenden durante el entrenamiento, pero cuando este finaliza, los datos de entrada pueden descartarse y únicamente mantener dichos parametros.\n",
    "\n",
    "\n",
    "Sin embargo, estos valores estimados son algo complicado de interpretar por que los números son demasiado grandes o demasiado chicos. Por lo que se busca normalizar cada fila de la matriz _logits_ para que sume 1 y cada valor este limitado entre cero y uno. Para esto se utiliza la función _softmax_ que se explica en la siguiente sección."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "El resultado de la clasificación _softmax_ puede interpretarse como la propabilidad logarítmica no normalizada de que un dato pertenezca a una clase dada. Por lo tanto, para obtener la probabilidad normalizada, es necesario exponenciar el score y dividirlo por el score total:\n",
    "\n",
    "\\begin{equation}\n",
    "P(y_i \\; | \\; x_i ; W) = (\\frac{e^{s_k}}{\\sum_{j}e^{s_j} }) \n",
    "\\end{equation}\n",
    "\n",
    "Considerando que $s_j$ es el \"score\" que se obtiene para la clase j del elemento i: $s_j = f(x_i,W)_j$. Al sumar todas las probabilidades de un elemento frente a las distintas clases se obtiene uno, por esto se dice que está normalizada.\n",
    "\n",
    "Se aplica la función _softmax_ al conjunto de datos utilizado _MNIST_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.nn.softmax(logits)\n",
    "y_pred_cls = tf.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de pérdida (_loss_) a optimizar\n",
    "\n",
    "La función de pérdida cuantifica las diferencias entre las categorias estimadas y las categorias verdaderas. Intuitivamente, la pérdida o costo va a ser alta si la clasificación del conjunto de entrenamiento es mala y baja en caso contrario. Existen distintos métodos para calcular la perdida, los dos mas utilizados son SVM (Multiclass Support Vector Machine) y Softmax.\n",
    "\n",
    "La funcion softmax (idealmente) nos devolvería una probabilidad de uno para la clase verdadera y probabilidad cero para las demás (falsas). Sin embargo, se necesita un unico valor que represente que tan mal se esta clasificando el conjunto de entrenamiento, por lo que se aplica la _cross-entropy_ a la función softmax.\n",
    "\n",
    "La entropía cruzada entre la distribución \"verdadera\" $p$ y la obtenida mediante la función softmax $q$ se define como:\n",
    "\n",
    "\\begin{equation}\n",
    "H(p,q) = - log({\\sum_{x}p(x) \\; log \\, q(x) }) \\\\\n",
    "q = \\frac{e^{s_k}}{\\sum_{j}e^{s_j} })\n",
    "\\end{equation}\n",
    "\n",
    "La entropia cruzada es una funcón cuyo resultado es siempre positivo y si la predicción del modelo es identica a la salida deseada, la entropia cruzada es cero. Es también pensar la entropia cruzada, en este campo, como la distancia entre ambas distribuciones.\n",
    "\n",
    "\\begin{equation}\n",
    "L_i = - log(\\frac{e^{s_k}}{\\sum_{j}e^{s_j} })\n",
    "\\end{equation}\n",
    "\n",
    "En la siguiente imagen puede verse un ejemplo numérico (del set de datos _CIFAR10_): \n",
    "\n",
    "<img src=\"images/softmax.png\" width=\"55%\">\n",
    "\n",
    "En la práctica, los términos $e^{s_k}$ y $\\sum_{j}e^{s_j}$ pueden volverse muy grandes debido a los exponenciales. Sin embargo, dividilos puede ser numéricamente inestable. Es importante utilizar algun tipo de normalización, por lo general se multiplican ambos términos por una constante $C$:\n",
    "\n",
    "\\begin{equation}\n",
    "(\\frac{e^{s_k}}{\\sum_{j}e^{s_j} }) = (\\frac{C \\, e^{s_k}}{C \\, \\sum_{j}e^{s_j} }) = (\\frac{e^{s_k + log C}}{\\sum_{j}e^{s_j + log C} })\n",
    "\\end{equation}\n",
    "\n",
    "C puede ser cualquier valor y se obtendrá el mismo resultado, pero se puede utilizar para mejorar la estabilidad computacional. Una elección común es $log C = -max_j f_j$. De esta forma, se desplazan todos los valores de forma tal en que el mayor termina siendo $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   0.  nan]\n",
      "[  5.75274406e-290   2.39848787e-145   1.00000000e+000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n",
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "f = np.array([123, 456, 789]) # example with 3 classes and each having large scores\n",
    "p = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup\n",
    "print(p)\n",
    "# instead: first shift the values of f so that the highest number is 0:\n",
    "f -= np.max(f) # f becomes [-666, -333, 0]\n",
    "p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularización\n",
    "\n",
    "Con el objetivo de diferenciar distintos conjuntos de parámetros que den como resultado perdidas $L$ similares, se agrega un término denominado \"pérdida de regularización\":\n",
    "\n",
    "\\begin{equation}\n",
    "L = \\frac{1}{N} \\; \\sum_{i} \\; L_i \\; + \\; \\lambda R(W)\n",
    "\\end{equation}\n",
    "\n",
    "Existen distintos métodos para realizar esta penalización, uno de los mas utilizados es la norma L2:\n",
    "\n",
    "\\begin{equation}\n",
    "R(W) = \\sum_{k} \\sum_{l} W^2_{k,l}\n",
    "\\end{equation}\n",
    "\n",
    "Como puede verser en la formula anterior, el objetivo es penalizar los pesos $W$ grandes por sobre los pequeños. Esto implica una mayor generalidad en el algoritmo, ya que, ninguna dimensión de entrada puede llegar a tener una influencia demasiado grande en los score por si sola. Se prefiere pequeñas ponderaciones en todas las dimenciones frente a algunas pocas muy fuertes, esto disminuye la tendencia al _overfitting_. \n",
    "Ejemplo: \n",
    "\n",
    "Si se considera la sigunte entrada $x$ y dos pesos $w_1$ y $w_2$:\n",
    "\\begin{align}\n",
    "x &= [1,1,1,1]\\\\\n",
    "w_1 &= [1,0,0,0]\\\\\n",
    "w_2 &= [0.25,0.25,0.25,0.25]\\\\\n",
    "w_1^T &= w_2^T = 1\\\\\n",
    "\\end{align}\n",
    "\n",
    "Como puede verse, todos llevan la misma pérdida, sin embargo, la penalidad **L2** debida al peso $w_1$ es 1 mientras que la debida al $w_2$ es 0.25.\n",
    "\n",
    "$\\lambda$ es un hiperparámetro que pondera la pérdida. Es dificil de predecir y por lo general se obtiene mediante cross-validation. Como se mencionó anteriormente, la función softmax provee las \"probabilidades\" para cada clase, sin embargo, estas probabilidades pueden ser más difusas o abruptas dependiendo del coeficiente de regulación $\\lambda$. Por ejemplo, si se supone que las porbabilidades logarítmicas no regularizadas para tres categorias son $[1, -2, 0]$, con la función softmax se calcularía:\n",
    "\n",
    "\\begin{equation}\n",
    "[1, -2, 0] \\rightarrow [e^1, e^{-2}, e^0] = [2.71,0.14,1] \\rightarrow [0.7,0.04,0.26]\n",
    "\\end{equation}\n",
    "\n",
    "Ahora, si el parametro $\\lambda$ se aumenta, los pesos $W$ mas grandes serían penalizados y por lo tanto, como resultado se obtendrían pesos de menor magnitud. Por ejemplo, si disminuyen a la mitad $[0.5, -1, 0]$, la funcion sofmax computaría:\n",
    "\n",
    "\\begin{equation}\n",
    "[0.5, -1, 0] \\rightarrow [e^{0.5}, e^{-1}, e^0] = [1.65,0.37,1] \\rightarrow [0.55,0.12,0.33]\n",
    "\\end{equation}\n",
    "\n",
    "Donde las probabilidades ahora son mas difusas. En el limite, donde los pesos tienden a numeros chicos debido a penalidades muy fuertes de $\\lambda$, las probabilidades tienden a la uniformidad.\n",
    "\n",
    "Agregando el término de regularización a la ecuación de pérdida, se obtiene:\n",
    "\n",
    "\\begin{equation}\n",
    "L_i = - log(\\frac{e^{s_k}}{\\sum_{j}e^{s_j} }) + \\sum_{k} \\sum_{l} W^2_{k,l}\n",
    "\\end{equation}\n",
    "\n",
    "The cross-entropy is a performance measure used in classification. The cross-entropy is a continuous function that is always positive and if the predicted output of the model exactly matches the desired output then the cross-entropy equals zero. __The goal of optimization is therefore to minimize the cross-entropy so it gets as close to zero as possible by changing the weights and biases of the model.__\n",
    "\n",
    "TensorFlow contiene funciones para calcular la entropia cruzada. Notar que se utilizan los valores de la variable logits por que se calcula el softmax internamente. La siguiente API aplica la función _softmax_ y la entropia cruzada al resultado de $Wx_i \\, + \\, b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                        labels=y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez calculada la entropia cruzada para la clasificación de cada imagen, es decir, se midió que tan bien el modelo se comporta para cada imagen individualmente. Sin embargo, se necesita un valor que pueda analizar el comportamiendo del modelo para todo el conjunto de entrenamiento. Entonces, simplemente se obtiene la media de la entropia cruzada de cada imagen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización\n",
    "\n",
    "A partir de las funciones de _score_ y _loss_, es necesario generar un proceso para elegir el conjunto de parámetros $W$ que minimice la función de pérdida. La optimización es un proceso estocastico e iterativo, donde en cada paso se intenta minimizar la función de pérdida. Para computar la mejor dirección en la que se debe modificar el conjunto de parámetros $W$ y así minimizar la función de pérdida, se utiliza el gradiente de la función de perdida. En una dimensión, la pendiente de la función es el ritmo de cambio instantaneo en cualquier punto, el gradiente es una generalizacion de la pendiente para funciones con más de una dimensión. Es decir, es un vector de pendientes para cada dimensión en el espacio de entrada (vector de derivadas parciales).\n",
    "\n",
    "### Gradient Decent\n",
    "\n",
    "Obtener el gradiente de la función de perdidas implica obtener la derivada parcial respecto a cada uno de los parametros $W$. El procedimiento de evaluar el gradente y luego actualizar los parámetros se denomina _Gradient Descent_. El siguiente código muestra una versión simplificada (_vanilla_) del _loop_ que se encuentra en todas las bibliotecas de redes neuronales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla Gradient Descent\n",
    "\n",
    "while True:\n",
    "  weights_grad = evaluate_gradient(loss_fun, data, weights)\n",
    "  weights += - step_size * weights_grad # perform parameter update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando el conjunto de entrenamiento es muy grande, es muy costoso evaluar la función de pérdida en todos los datos únicamente para actualizar los parámetros. Una aproximación muy utilizadas es computar el gradiente para un lote aleatorio de datos que cambia en cada iteración (_Minibatch Gradient Descent_). El tamaño de este lote es un hiperparámetro, no es común que se obtenga por validación cruzada sino que depende de la cantidad de memoria disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla Minibatch Gradient Descent\n",
    "\n",
    "while True:\n",
    "  data_batch = sample_training_data(data, 256) # sample 256 examples\n",
    "  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)\n",
    "  weights += - step_size * weights_grad # perform parameter update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La versión extrema de este optimizador, es cuando el subconjunto (_batch_) únicamente contiene un ejemplo, este proceso se denomina _stochastic Gradient Descent (SGD)_. Es poco común de ver, debido a las optimizaciones de código vectorial. Es computacionalmente mas efectivo evaluar el gradiente de 100 ejemplos que el gradiente de un ejemplo 100 veces. Suele referirse (erroneamente) tambien con este nombre al método _Minibatch Gradient Descent_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Como se mencionó, para poder aplicar _Gradient Decent_ es necesario contar con el gradiente de la función de pérdidas, dado que es la forma más eficiente de aplicar este algoritmo de optimización. Sin embago, para algunas funciones de pérdidas, esto puede ser dificil de resolver. _Backpropagation_ es un método de computar gradientes de expresiones a través de la aplicación de la _chain rule_, donde complejas funciones se resuelven dividiendolas en expresiones más sencillas hasta que cada una de ellas es fácil de resolver.\n",
    "Si se considera la expresión $f(x,y,z) = (x + y)z$ a modo de ejemplo, lo primero que se hace es dividirla en dos: $q = x + y$ y $f = qz$. Calcular las derivas de ambas expresiones por separado es sencillo: \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\partial q} &= z \\\\\n",
    "\\frac{\\partial f}{\\partial z} &= q \\\\\n",
    "\\frac{\\partial q}{\\partial x} &= 1 \\\\\n",
    "\\frac{\\partial q}{\\partial y} &= 1 \\\\\n",
    "\\end{align}\n",
    "\n",
    "La _chain rule_ implica que se deben conectar estas expresiones mediante muliplicación:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\partial x} &= \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x} \\\\\n",
    "\\frac{\\partial f}{\\partial y} &= \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial y} \\\\\n",
    "\\frac{\\partial f}{\\partial z} &= \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial z} \\\\\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"images/backprop.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q = 3\n",
      "f = -12\n",
      "df/dz = 3\n",
      "df/dq = -4\n",
      "df/dx = -4.0\n",
      "df/dy = -4.0\n"
     ]
    }
   ],
   "source": [
    "# set some inputs\n",
    "in_x = -2; in_y = 5; in_z = -4\n",
    "\n",
    "# perform the forward pass\n",
    "q = in_x + in_y # q becomes 3\n",
    "f = q * in_z # f becomes -12\n",
    "print(\"q = \" + str(q) + \"\\nf = \" + str(f))\n",
    "# perform the backward pass (backpropagation) in reverse order:\n",
    "# first backprop through f = q * z\n",
    "dfdz = q # df/dz = q, so gradient on z becomes 3\n",
    "dfdq = in_z # df/dq = z, so gradient on q becomes -4\n",
    "print(\"df/dz = \" + str(dfdz) + \"\\ndf/dq = \" + str(dfdq))\n",
    "# now backprop through q = x + y\n",
    "dfdx = 1.0 * dfdq # dq/dx = 1. And the multiplication here is the chain rule!\n",
    "dfdy = 1.0 * dfdq # dq/dy = 1\n",
    "print(\"df/dx = \" + str(dfdx) + \"\\ndf/dy = \" + str(dfdy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada compuerta en el diagrama a partir de las entradas computa dos cosas:\n",
    "1. El valor de salida (_forward_) \n",
    "2. El gradiente local (_backward_)\n",
    "\n",
    "Se debe notar que las compuertas pueden hacer esto completamente independientemente del resto del circuito. Entonces, una vez que finalizó la pasada hacia adelante (_forward_), durante _backpropagation_ se calcula el gradiente local al yendo de atrás hacia adelante. La _chain rule_ establece que se debe multiplicar los gradientes locales hacia todas las entradas.\n",
    "\n",
    "### Función sigmoid\n",
    "\n",
    "Una función de activación que se utiliza en la neuronas, como se vera más adelante, es la función sigmoid:\n",
    "\n",
    "\\begin{equation}\n",
    "f(w,x) =  \\frac{1}{1 + e^{-(w_0x_0+w_1x_1+w_2)}}\n",
    "\\end{equation}\n",
    "\n",
    "cuyo grafo se muestra a continuación:\n",
    "\n",
    "<img src=\"images/sigmoidGrafo.png\" width=\"55%\">\n",
    "\n",
    "Si simplificamos la función con una única entrada para facilitar los calculos, se obtiene:\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma (x) &=  \\frac{1}{1 + e^{-x}} \\\\\n",
    "\\frac{d\\sigma (x)}{dx} &= \\frac{e^{-x}}{(1 + e^{-x})^2} = \\left( \\frac{1 + e^{-x} - 1}{1 + e^{-x}}\\right) \\left( \\frac{1}{1 + e^{-x}} \\right) = (1 - \\sigma(x))\\sigma(x)\n",
    "\\end{align}\n",
    "\n",
    "Como puede verse, es posible simplificar el gradiente antes de aplicar _backpropagation_, la neurona quedaria como se ve en el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [2,-3,-3] # assume some random weights and data\n",
    "x = [-1, -2]\n",
    "\n",
    "# forward pass\n",
    "dot = w[0]*x[0] + w[1]*x[1] + w[2] #Intermediate varible for dot product between w and x\n",
    "f = 1.0 / (1 + math.exp(-dot)) # sigmoid function\n",
    "\n",
    "# backward pass through the neuron (backpropagation)\n",
    "ddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivation\n",
    "dx = [w[0] * ddot, w[1] * ddot] # backprop into x\n",
    "dw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actualización de parámetros\n",
    "\n",
    "Una vez que se calculó el gradiente mediante _backpropagation_, se lo utiliza para actualizar los parámetros. Existen varias formas de realizar esta actualización, la forma más sencilla es cambiarlos en la dirección contraria del gradiente, ya que, el gradiente indica la dirección en que aumenta pero se necesita minimizar la función de pérdida. Asumiendo que $x$ es el vector de parámetros y el gradiente calculado es $dx$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla update\n",
    "x += -learning_rate * dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donde $learning\\_rate$ es un hiperparámetro constante. Cuando se evalua el set de entrenamiento completo, y $learning\\_rate$ es lo suficientemente estamos seguros que la actualización mejorará la función de pérdida.\n",
    "\n",
    "### Actualización mediante _Momentum_\n",
    "\n",
    "Este método logra una mejor convergencia en redes neurnoales profundas y que modela la función de pérdida como la altura de un terreno montañoso. Por lo tanto existe una energía potencial $U = mgh$, es decir, la energía potencial es directamente proporcional a la altura. Se inicializa los parámetros con valores aleatorios, lo que equivale a una partícula con velocidad incial cero en un punto cualquiera. El proceso de optimización puede verse como una simulación de una partícula (vector de parámetros) movendose por el terreno.\n",
    "\n",
    "Como la fuerza en la partícula esta relacionada con el gradiente de la energía potencial $F = -\\nabla U$, y además, $F = ma$, por lo que el gradiente negativo es proporcional a la aceleracion de la partícula. A diferencia de la actualización SGD, el gradiente afecta indirectamente la posición de la partícula, a través de la velocidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum Update\n",
    "v = mu * v - learning_rate * dx # intagrate velocity\n",
    "x += v # integrate position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable de la velocidad $v$ se inicializa a cero. Aunque a la variable $mu$ se la llama _momentum_, fisicamente es mas concistente con el concepto del coeficiente de fricción. Esta variable, amortigua la velocidad y reduce la energía cinética del sistema, o de otra forma, la partícula no se frenaria al llegar al fondo de la colina. Existe una mejora de este método de actualización llamado _Nesterov Momentum_ el cual garantiza mejor convergencia.\n",
    "\n",
    "### Métodos adaptativos de _learning rate_ por parámetros\n",
    "\n",
    "Los métodos anteriores modifican el _learning rate_ en forma global y equitativa para todos los parámetros.\n",
    "\n",
    "#### Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad\n",
    "cache += dx**2\n",
    "x += -learning_rate * dx / (np.sqrt(cache) + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable $cache$ tiene el mismo tamaño que $dx$, se utiliza para registrar por parámetros la suma del cuadrado del gradiente. Este valor luego se utiliza para normalizar en cada paso la actualización de los parámetros, de a uno por vez. Se debe notar que los pesos que reciben gradientes grandes tendrán su efecto en un _learning rate_ reducido, mientras que aquellos pesos que recibiran gradientes chicos o infrecuentes, tendrán su efecto en un _learning rate_ aumentado. El coeficiente _eps_ evita que haya diviciones por cero.\n",
    "Este método, en el caso de las redes neuronales profundas, generalmente tienen un efecto demasiado agresivo y detiene le aprendisaje demasiado temprano.\n",
    "\n",
    "#### RMSprop\n",
    "RMSprop ajusta el método Adagrad para reducir la agresividad monotónica del desenso del _learning rate_. En particular, utiliza un promedio móvil del cuadrado del gradiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = decay_rate * cache (1 - decay_rate) * dx**2\n",
    "x += -learning_rate * dx / (np.sqqrt(cache) + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$decay\\_rate$ es un hiperparámetro (típicamente entre los valores 0.9, 0.99,0.999). Aunque la actualización de $x$ es idéntica al método Adagrad, el calculo de $cache$ es completamente distinto. RMSprop continúa modulando el _learning rate_ de cada peso basado en la magnitud de los gradientes, pero a diferencia del Adagrad, las actualizaciones no se hacen monotónicamente mas chicas.\n",
    "\n",
    "#### Adam\n",
    "\n",
    "Este método combina RMSprop con _momentum_. La actualización, en forma simplificada es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = beta1*m + (1 - beta1) * dx\n",
    "v = beta2*v + (1 - beta2) * dx**2\n",
    "x += -learning_rate * m /(np.sqrt(v) + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La actualización es igual a la  del RMSProp, excepto por que se utiliza una versión suavizada de $m$ en lugar del vector gradiente $dx$ que puede ser muy ruidoso. Los valores recomendados de los hiperparametros son:\n",
    "- $eps = 1e-8$\n",
    "- $beta1 = 0.9$\n",
    "- $beta2 = 0.999$\n",
    "\n",
    "La actualización Adam completa incluye un mecanismo de corrección de sesgo (_bias_) que compensa al inicio, cuando los vectores $m$,$v$ son cero. Haciendo que la actualización sea función de las iteraciones.\n",
    "__Este algoritmo es el que se recomienda utilizar por defecto__, aunque en ciertos casos es recomendable probar algunos otros como _SGD + Nesterov Momentum_. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/opt1.gif\" width=\"40%\">\n",
    "<img src=\"images/opt2.gif\" width=\"40%\">\n",
    "\n",
    "\n",
    "Image credit: Alec Radford (https://twitter.com/alecrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente código, se selecciona que optimizador se utilizará en el proceso de optimización. Luego, se implementa una función llamada _optimize_ quee nos permite controlar cuantas iteraciones de optimización se realizán. En cada iteración se se obtiene un nuevo _batch_ de datos de entrenamiento y luego _TensorFlow_ ejecuta la optimización para esas muestras del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.5).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(num_iterations):\n",
    "    for i in range(num_iterations):\n",
    "        # Get a batch of training examples.\n",
    "        # x_batch now holds a batch of images and\n",
    "        # y_true_batch are the true labels for those images.\n",
    "        x_batch, y_true_batch = data.train.next_batch(batch_size)\n",
    "        \n",
    "        # Put the batch into a dict with the proper names\n",
    "        # for placeholder variables in the TensorFlow graph.\n",
    "        # Note that the placeholder for y_true_cls is not set\n",
    "        # because it is not used during training.\n",
    "        feed_dict_train = {x: x_batch,\n",
    "                           y_true: y_true_batch}\n",
    "\n",
    "        # Run the optimizer using this batch of training data.\n",
    "        # TensorFlow assigns the variables in feed_dict_train\n",
    "        # to the placeholder variables and then runs the optimizer.\n",
    "        session.run(optimizer, feed_dict=feed_dict_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sesión de TensorFlow \n",
    "\n",
    "\n",
    "Es importante notar que hasta ahora no se realizo ningun cálculo en TensorFlow, únicamente se crearon los objetos de TensorFlow. Para poder ejecutarlo, es necesario primero crear una sesión e inicializar las variables de pesos y sesgos en cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se mencionó anteriormente, el set de entrenamiento contiene 50.000 imágenes, esto ocaciona que el tiempo de calculo del gradiente para todas estas imágenes sea demasiado grande. Por lo que se decirde usar un sub-conjunto (_batch_) de 100 imágenes para cada iteración de la optimización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, se crea la variable $feed\\_dict\\_test$ la cual es usada como entrada al grafo de _TensorFlow_, se deben utilizar los nombres correctos de los nodos _placeholder_ creados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict_test = {x: data.test.images,\n",
    "                  y_true: data.test.labels,\n",
    "                  y_true_cls: data.test.cls}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presición en la clasificación antes de realizar optimizaciones\n",
    "\n",
    "La presición en el set de test es del 9.8%, esto se debe a que el modelo únicamente fue inicializado pero no optimizado. El 9.8% de las imágenes son el dígito cero, como el modelo se inició en cero, este es el único dígito que clasifica correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test-set: 9.8%\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAD5CAYAAACj3GcTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noEpTRCSAKk0RkQD/H2eeucmW8uENAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c371c0ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_example_errors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presición en la clasificación luego de una iteración de optimización\n",
    "\n",
    "Luego de una única iteración, el modelo incrementa su presición del 9.8% al 43.5%. Esto significa que clasifica mal 6 imagenes de cada 10, como puede verse a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test-set: 43.5%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAD5CAYAAACj3GcTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c3b395eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimize(num_iterations=1)\n",
    "print_accuracy()\n",
    "plot_example_errors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si analizamos este clasificador como un comparador entre una imagen y un _template_, cada fila de la matriz de pesos (_weights_) $W$ es un _template_ para cada una de las clases. Si a estas filas le damos la forma de una imagen, podemos visualizar la forma en que el clasificador esta trabajando. A continuación se grafican estos pesos, donde los positivos estan en rojo y los negativos en azul. Estos pesos pueden entenderse intuitivamente como un filtro de imagen.\n",
    "\n",
    "Por ejemplo, los pesos utilizados para determinar si una imagen muestra el digito cero, tienen una reacciòn positica a una imagen similar de un circulo (rojo) y una reacción negativa al centro del circulo y al exterior (azul).\n",
    "\n",
    "Notar que los pesos se parecen a los digitos que se suponen deben reconocer. Esto es por que se realizó una única iteración y, por lo tanto, los pesos únicamente fueron entrenados con 100 imágenes. Luego de entrenarlo con miles, estos pesos se vuelven difícil de interpretar por que intentan reconocer todas las variaciones de como distintas personas escriben los números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD5CAYAAAAZf+9zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsBuwYRhGRtgN2DAMIyPsBmwYhpER/z8BOmJPDOm67gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd5ef316240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presición en la clasificación luego de 1000 iteraciones de optimización\n",
    "\n",
    "Luego de 1000 iteraciones, el modelo clasifica mal una de cada diez imágenes aproximadamente. Como se demuestra a continuación, alguna de las clasificaciones erroneas están justificadas, ya que, es muy difícil diferenciar incluso para las personas, mientras que otras son bastante obvias y deberían haber sido clasificadas correctamente. Sin embargo, este modelo simple no puede comportarse mucho mejor, es necesario un modelo más complejo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test-set: 91.8%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAD5CAYAAACj3GcTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQIWmiEgEKjRFRCJQoSkiEsH/B/mN5CsJB61OAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd5b967aeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We have already performed 10 iterations.\n",
    "optimize(num_iterations=990)\n",
    "print_accuracy()\n",
    "plot_example_errors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo ahora fue entrenado con 1000 iteraciones de optimización, cada iteración usando 100 imagenes del set de entrenamiento. Debido a la gran variedad de imágenes, los pesos ahora se volvieron muy difícil de interprtar y podriamos preguntarnos si el modelo realmente entendió como los digitos se componen de lineas o si simplemente el modelo memorizó diferentes variaciones de pixeles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD5CAYAAAAZf+9zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwAaDwZAT7AVsMBgMOeH/A7o7ukv4FxLrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd5ef472ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se grafica la matriz de confusión que nos permite analizar en detalle las clasificaciones erroneas. Por ejemplo, nos muestra que las imágenes con el digito 5, a veces son confundidas con digitos como el 3, 8, o 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 961    0    1    1    0    2   10    2    3    0]\n",
      " [   0 1104    3    2    0    1    4    2   19    0]\n",
      " [  15    5  914    9    7    1   16   12   43   10]\n",
      " [   4    1   23  888    0   29    5   15   33   12]\n",
      " [   1    1    6    1  890    0   12    2   10   59]\n",
      " [  12    3    5   25    8  745   24    6   56    8]\n",
      " [  11    3    4    1    9    7  917    1    5    0]\n",
      " [   2    6   23    6    7    1    0  929    3   51]\n",
      " [   6    5    5   11    8   13   12   11  892   11]\n",
      " [  10    4    1    5   15    8    1   12   13  940]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAEmCAYAAABcYEo9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHYlJREFUeJzt3X+QXWWd5/H3Jx0ICaAgHVMhPySOWZxIlYK9GZSRReIP\nUBacrdEKNWpUZuKO6ICO68DsbrG7VdTqjmX5Y0adCGoc+WGMUqYcRTGDMloSDCEKISCREEgMJPEX\niAgk+e4f52m9id3Jud3nueeecz8v6lSfe+65z/ecdPPtp5/z/FBEYGZmvTel7gswMxtUTsBmZjVx\nAjYzq4kTsJlZTZyAzcxq4gRsZlYTJ2Azs5o4AZuZ1cQJ2MysJlPrvoBOmnZsaMYJWWOc+tzhrOVb\n/8g9xlOZy2+DbdseYM+ePZX+Uw094zkRe58ofX48sfsbEXFOlddQlf5KwDNOYNrZ/zNrjO994aKs\n5Vv/2L8/bwqeMsUp+HDO+JORysuMvb9l2vOXlj7/t3d8rG9rXX2VgM3MDkuA2vHLzwnYzJpH7Xh8\n5QRsZs3jGrCZWR3kGrCZWW1cAzYzq4FwDdjMrB5qTQ04668RSedIulfSFkmX5YxlZgNEU8pvfSzb\n1UkaAv4JOBdYBFwoaVGueGY2QKTyWx/L+ethMbAlIu6PiKeA64ELMsYzs4Eg14BLmAM81PF6ezp2\nAEnLJa2XtD6efCzj5ZhZK4yOhGtBDbj2h3ARsQJYATDl+JNyz59iZm3Q5zXbsnIm4B3AvI7Xc9Mx\nM7NJaM9AjJx38QNgoaQFko4ElgJrMsYzs0ExReW3PpatBhwReyW9E/gGMAR8OiI25YpnZgPCAzHK\niYivAV/LGcPMBlCfP1wrq/aHcGZm3WlPG7ATsJk1T0tqwO34NWJmg6XCgRiSPi1pl6S7Oo49S9JN\nku5LX4/veO/yNL3CvZJe3XH8xZLuTO99VDr8bwknYDNrlm4GYZSrKX8WOHjRzsuAtRGxEFibXpOm\nU1gKvCB95uNp2gWATwB/BSxM22EXAnUCNrPmmTJUfjuMiLgF+PlBhy8AVqb9lcDrOo5fHxFPRsRW\nYAuwWNJs4BkRcWtEBPC5js+My23AZtYwXT+EG5a0vuP1ijQC91BmRcTOtP8wMCvtzwFu7ThvdIqF\np9P+wccPyQnYzJqnu4dweyJiZKKhIiIkZZkmoa8S8KnPHeZ7X7goa4zj/+M7s5b/ix/8Y9byrbwp\nfT4Kqoy9+/ZnLX/qUANbIXszEOMRSbMjYmdqXtiVjo83xcKOtH/w8UNq4L++mQ22nkxHuQZYlvaX\nAV/pOL5U0jRJCygett2WmiselXR66v3w5o7PjKuvasBmZqVU2A9Y0nXAWRRtxduBK4D3A6skXQRs\nA94AEBGbJK0C7gb2AhdHxL5U1DsoelRMB76etkNyAjaz5qmwCSIiLhznrSXjnH8lcOUYx9cDp3QT\n2wnYzJqnJSPhnIDNrFnkuSDMzOrjGrCZWT1KTLPQCE7AZtYoxZqc7UjA2RpSxpphyMxs0tTl1sdy\ntmR/lhKzAZmZdUdI5bd+lnNNuFsknZSrfDMbXP2eWMuqvQ1Y0nJgOcC8+fNrvhoza4K2JODaO9NF\nxIqIGImIkZnDM+u+HDNrADdBmJnVoQEP18pyAjazRhH9X7MtK2c3tOuA7wMnS9qeZhUyM5s0N0Ec\nxiFmGDIzm5R+T6xluQnCzBrHCdjMrA5+CGdmVh/XgM3MatCmXhBOwGbWOGrBitfgBGxmTSM3QWQR\nwL79kTXGz2/7WNbyT3zbtVnLB9h+Vd4efr342e7F/0C5f5Z6kQL2Zr6HKcpbfq7SnYDNzGriBGxm\nVgM/hDMzq1M78q8TsJk1jB/CmZnVxwnYzKwmTsBmZnVpR/51Ajaz5mlLDTjnhOzzJN0s6W5JmyRd\nkiuWmQ2ObiZj7/dEnbMGvBf424jYIOlY4HZJN0XE3RljmtkA6PfEWlbOFTF2AjvT/mOSNgNzACdg\nM5uUtiTgnixLL+kk4FRg3RjvLZe0XtL6PXt29+JyzKzp1MXWx7InYEnHAF8CLo2IRw9+PyJWRMRI\nRIwMD8/MfTlm1gJuAy5B0hEUyfeaiPhyzlhmNiBaNBIuZy8IAVcDmyPiQ7nimNlgEcWUqWW3UmVK\n7069te6SdJ2koyQ9S9JNku5LX4/vOP9ySVsk3Svp1RO9l5xNEGcAbwLOlrQxba/JGM/MBkK13dAk\nzQH+BhiJiFOAIWApcBmwNiIWAmvTayQtSu+/ADgH+LikoYncSc5eEN+l75vAzayJMrRATAWmS3oa\nmAH8FLgcOCu9vxL4NvB3wAXA9RHxJLBV0hZgMfD9boP2pBeEmVmVqqwBR8QO4IPAgxRdZ38VEd8E\nZqXutAAPA7PS/hzgoY4itqdjXXMCNrNm6aL9N+Xf4dGurmlbfkBxRdvuBcAC4ETgaElv7DwnIoIM\nKyx5LggzaxQBU7pbFXlPRIwc4v1XAFsjYjeApC8DLwUekTQ7InZKmg3sSufvAOZ1fH5uOtY114DN\nrHEq7gXxIHC6pBmp99YSYDOwBliWzlkGfCXtrwGWSpomaQGwELhtIvfhGrCZNYu6rgEfUkSsk7Qa\n2EAxh80dwArgGGCVpIuAbcAb0vmbJK2imFZhL3BxROybSGwnYDNrlKIfcLXdICLiCuCKgw4/SVEb\nHuv8K4ErJxvXCdjMGqb/hxiX1VcJWMBQhX9a1OHBTy3NHmPBO1ZnLX/bJ1+ftXyAp/fuzx5j6lDe\nn6X9lT8T/0NTWpJoqtaWf5a+SsBmZmW4BmxmVocu5njod07AZtYoOR7C1cUJ2MwapyX51wnYzJrH\nNWAzs5q0JP86AZtZw7RoRQwnYDNrlNEVMdogWwKWdBRwCzAtxVmdhvuZmU2CR8KV8SRwdkT8Oi3O\n+V1JX4+IWzPGNLMB0JL8m3VJogB+nV4ekbYeDN40s7ZrSw0463zAkoYkbaSYyPimiFg3xjnLR2eq\n371nd87LMbM26H5FjL6VNQFHxL6IeBHFjPGLJZ0yxjkrImIkIkZmDs/MeTlm1gKjI+GqWhOuTj1Z\nESMifgncTLGEs5nZpDgBH4akmZKOS/vTgVcC9+SKZ2aDoy1NEDl7QcwGVkoaokj0qyLiqxnjmdmA\n6PeabVk5e0H8CDg1V/lmNqAaULMtyyPhzKxR5IEYZmb1aUn+dQI2s+Zpy1p5TsBm1igSTGn44r2j\nnIDNrHFakn+dgM2sefwQzsY01INfzds++fqs5c9/+6qs5QM8+M9vyB5j//68cz/14nudO8ITT+/L\nWv7+yPM9aEn+dQI2s2YRRVe0NnACNrPGcRuwmVkdGjDJTllOwGbWOC3Jv07AZtYswgMxzMxq05L8\n6wRsZs3jNmAzsxo0YaL1srIn4DQh+3pgR0SclzuembVfW9qAe7Em3CXA5h7EMbMBoS62fpZ7Wfq5\nwGuBq3LGMbPBUvWinJKOk7Ra0j2SNkt6iaRnSbpJ0n3p6/Ed518uaYukeyW9eqL3kbsG/GHgfcD+\nzHHMbEAU3dDKbyV9BLgxIp4PvJDir/bLgLURsRBYm14jaRGwFHgBxUrvH09NrV3LuSryecCuiLj9\nMOctl7Re0vrde3bnuhwza4suar9lasCSngmcCVwNEBFPRcQvgQuAlem0lcDr0v4FwPUR8WREbAW2\nAIsncis5a8BnAOdLegC4Hjhb0ucPPikiVkTESESMzByemfFyzKwtulyWfni0kpe25QcVtwDYDXxG\n0h2SrpJ0NDArInamcx4GZqX9OcBDHZ/fno51rXQvCEnTIuLJsudHxOXA5emzZwHvjYg3dn2FZmYH\n6bIf8J6IGDnE+1OB04B3RcQ6SR8hNTeMioiQVPncmoetAUtaLOlO4L70+oWSPlb1hZiZlZGhDXg7\nsD0i1qXXqykS8iOSZgOkr7vS+zuAeR2fn5uOda1ME8RHgfOAnwFExA+Bl3cTJCK+7T7AZlaVKtuA\nI+Jh4CFJJ6dDS4C7gTXAsnRsGfCVtL8GWCppmqQFwELgtoncR5kmiCkRse2gG8k7jb6Z2SFk6N/7\nLuAaSUcC9wNvpaigrpJ0EbANeANARGyStIoiSe8FLo6ICeXEMgn4IUmLgUhdLd4F/HgiwczMJkuq\nfiRcRGwExmonXjLO+VcCV042bpkE/NcUzRDzgUeAb6VjZma1aMlI5MMn4IjYRdHp2MysLwzMbGiS\nPgX8QfeLiDi4L52ZWXZCPVmRuhfKNEF8q2P/KODPOLATsplZ7wzSdJQR8YXO15L+BfhutisyMzuM\ngWmCGMMCfj8kr1IB7N9f+WCTA0xpwZ8ue/flndto2ydfn7V8gJd94NvZY9z83jOzll/9uKg/9Nun\n8/b4nHHkhOaQKS3XvL29mEe3F8q0Af+C37cBTwF+zkHD9MzMekUMSA1YxV2+kN8Ps9sfET34vW9m\nNr4W/CELHKYmn5Lt1yJiX9qcfM2sdhnmA65FmaaUjZJOzX4lZmYlFNNMVrsiRl3GbYKQNDUi9gKn\nAj+Q9BPgcYommIiI03p0jWZmB+j3mm1Zh2oDvo1iSrbze3QtZmal9HnFtrRDJWABRMRPenQtZmaH\nVcwH3I4MfKgEPFPSe8Z7MyI+lOF6zMwOaxD6AQ8BxzCJqTfTenCPUcwfvPcwy4KYmZXSkgrwIRPw\nzoj4PxXEeHlE7KmgHDMzJA1EE0Q77tDMWqcl+feQTSljzgTfpQC+Jen2MZaCBkDS8tHlovfs2V1B\nSDNru7YMxBi3BhwRP6+g/D+NiB2Sng3cJOmeiLjloDgrgBUAp714xCPtzOyQ2tQLIuvDxIjYkb7u\nAm4AFueMZ2aDQSq/9bNsCVjS0ZKOHd0HXgXclSuemQ2ILpofGtsEUYFZwA1pLPZU4NqIuDFjPDMb\nEGpJH4FsCTgi7qeYytLMrDJFG3DdV1GNnDVgM7MsnIDNzGrS79NMluUEbGaN4iYIM7O6CIZakoGd\ngM2sUVwDNjOrUUuagJ2AzaxpxBT3A7ax9OLp7NShvDH2788/Jcct7/tP2WOc+LZrs5a/8zN/kbV8\ngOlHDmUtP/fPa47ShWvAZmb1aMAQ47KcgM2scdoyG5oTsJk1ipsgzMxq5BqwmVlNWpJ/W7O6s5kN\nCFEkrrJb6XKlIUl3SPpqev0sSTdJui99Pb7j3MslbZF0r6RXT/RenIDNrFlUdJ8ru3XhEmBzx+vL\ngLURsRBYm14jaRGwFHgBcA7wcUkT6i/oBGxmjaMutlLlSXOB1wJXdRy+AFiZ9lcCr+s4fn1EPBkR\nW4EtTHC5tawJWNJxklZLukfSZkkvyRnPzNpvdFHOshswPLryetrGWqH9w8D7gP0dx2ZFxM60/zDF\nKj8Ac4CHOs7bno51LfdDuI8AN0bEn0s6EpiROZ6ZDYAun8HtiYiRccuSzgN2RcTtks4a65yICEmV\nDxHNloAlPRM4E3gLQEQ8BTyVK56ZDY6Ke0GcAZwv6TXAUcAzJH0eeETS7IjYKWk2sCudvwOY1/H5\nuelY13I2QSwAdgOfSU8Wr0qrIx9A0vLRPw327Nmd8XLMrB3KP4Ar8xAuIi6PiLkRcRLFw7V/i4g3\nAmuAZem0ZcBX0v4aYKmkaZIWAAuB2yZyJzkT8FTgNOATEXEq8DjpKWKniFgRESMRMTI8PDPj5ZhZ\nG+TqhjaG9wOvlHQf8Ir0mojYBKwC7gZuBC6OiH0TCZCzDXg7sD0i1qXXqxkjAZuZdSvXLG4R8W3g\n22n/Z8CScc67ErhysvGy1YAj4mHgIUknp0NLKH5jmJlNStXd0OqSuxfEu4BrUg+I+4G3Zo5nZm0n\nr4pcSkRsBMbt/mFm1q3RNuA28GQ8ZtY4rgGbmdWkHenXCdjMGkbAkGvAZmb1aEn+dQI2s6YRakkj\nhBOwmTWOa8AZCJiSeb3piMonNDrAvv15ywcYyvxv1Isf7l48xd75mb/IWv6sN/9L1vIBHvncm7KW\n/8RTExpBW1qO/x2KbmjtyMB9lYDNzA5LrgGbmdXGCdjMrCZ+CGdmVoNiSaK6r6IaTsBm1jiuAZuZ\n1cRtwGZmNXEN2MysBm1qA842raakkyVt7NgelXRprnhmNijU1X/9LFsNOCLuBV4EIGmIYtnmG3LF\nM7MB4YEYXVsC/CQitvUonpm1WEvyb88S8FLgurHekLQcWA4wb/78Hl2OmTVV0QbcjhScfWmltCDn\n+cAXx3o/IlZExEhEjMwcnpn7csysBbwqcnnnAhsi4pEexDKzQdDvmbWkXiTgCxmn+cHMbCL6vXdD\nWVmbICQdDbwS+HLOOGY2WKTyWz/LWgOOiMeBE3LGMLPB0+d5tTSPhDOz5mlJBnYCNrNGKXo3tCMD\nOwGbWbM0oG23LCdgM2scJ2Azs1r0/yQ7ZTkBm1njuAZsZlaDJgwxLquvEnAAEVH3ZUzKUA9mis79\nT9SL2sW+/fm/z7lvY+dn35g5Asx/+6qs5T/widdnLT/bz1JLMnD2yXjMzKpW5YTskuZJulnS3ZI2\nSbokHX+WpJsk3Ze+Ht/xmcslbZF0r6RXT/Q+nIDNrHEqHoq8F/jbiFgEnA5cLGkRcBmwNiIWAmvT\na9J7S4EXAOcAH0+LTnTNCdjMGqfK6SgjYmdEbEj7jwGbgTnABcDKdNpK4HVp/wLg+oh4MiK2AluA\nxRO5DydgM2uWbrJvkYGHJa3v2JaPW7R0EnAqsA6YFRE701sPA7PS/hzgoY6PbU/HutZXD+HMzMro\nsh/wnogYOWyZ0jHAl4BLI+JRdbRfRERIqvzJsWvAZtYoovrpKCUdQZF8r4mI0elzH5E0O70/G9iV\nju8A5nV8fG461jUnYDNrnCrbgFVUda8GNkfEhzreWgMsS/vLgK90HF8qaZqkBcBC4LaJ3IebIMys\neartB3wG8CbgTkkb07G/B94PrJJ0EbANeANARGyStAq4m6IHxcURsW8igbMmYEnvBv6SYozFncBb\nI+K3OWOaWftVORdERHyX8VP6knE+cyVw5WRjZ2uCkDQH+BtgJCJOAYYo+s6ZmU2KlyQqX/50SU8D\nM4CfZo5nZgOgz/NqadlqwBGxA/gg8CCwE/hVRHzz4PMkLR/tn7dnz+5cl2NmbVLlU7ga5WyCOJ5i\nxMgC4ETgaEl/MHtJRKyIiJGIGBkenpnrcsysJUaXJKpqLog65eyG9gpga0TsjoinKZamf2nGeGY2\nCLpo/+33NuCcCfhB4HRJM1I/uyUUY6zNzCalJS0Q+R7CRcQ6SauBDRR95e4AVuSKZ2YDpN8za0lZ\ne0FExBXAFTljmNmg6f+23bI8Es7MGqff23bLcgI2s0ZpQttuWU7AZtY4akkV2AnYzBqnJfnXCdjM\nmqcl+dcJ2MwapgEDLMrquwQclS/6caApU9rwncv7j7Q/8/cAoBffhtzthPt78A+19ROvz1r+rDd/\nLmv5v9n6s0wlt+H/4z5MwGZmhzK6JFEbOAGbWeO0JP86AZtZ87gGbGZWEw9FNjOrSzvyrxOwmTVP\nS/KvE7CZNUsTJlovywnYzBqnLW3AOVfEQNIlku6StEnSpTljmdkAacmSGDkX5TwF+CtgMfBC4DxJ\nz8sVz8wGR0vyb9Ya8B8D6yLiNxGxF/gO8F8yxjOzAeFFOQ/vLuBlkk6QNAN4DTAvYzwzGwjdLErf\n3xk456KcmyV9APgm8DiwEdh38HmSlgPLAebNn5/rcsysJdo0F0TWh3ARcXVEvDgizgR+Afx4jHNW\nRMRIRIwMD8/MeTlmZn0lazc0Sc+OiF2S5lO0/56eM56ZDYa21IBz9wP+kqQTgKeBiyPil5njmdkA\n6Pe23bKyJuCIeFnO8s1sADWgd0NZHglnZo3ShP69ZTkBm1nztCQDOwGbWeNMaUkbhBOwmTVOO9Jv\n5n7AZmZZVDwZhKRzJN0raYuky3Jc8licgM2scaociixpCPgn4FxgEXChpEWZbwFwAjazhhkdilzh\nZDyLgS0RcX9EPAVcD1yQ8RZ+p6/agO/YcPueo6dN2dbFR4aBPbmupwfltyWG72FwYnRb/nOqvoAN\nG27/xvQjNNzFR46StL7j9YqIWNHxeg7wUMfr7cCfTOYay+qrBBwRXU0GIWl9RIzkup7c5bclhu9h\ncGL04h4OJyLOqTN+ldwEYWaDbgcHTpU7Nx3LzgnYzAbdD4CFkhZIOhJYCqzpReC+aoKYgBWHP6Wv\ny29LDN/D4MToxT30VETslfRO4BvAEPDpiNjUi9iKiF7EMTOzg7gJwsysJk7AZmY1aWQCzj1sUNKn\nJe2SdFfVZXfEmCfpZkl3S9ok6ZKKyz9K0m2SfpjK/99Vlt8RZ0jSHZK+mqn8ByTdKWnjQX05q4xx\nnKTVku6RtFnSSyou/+R0/aPbo5IurTjGu9P3+S5J10k6qsryU4xLUvmbqr7+gRURjdooGsl/AjwX\nOBL4IbCo4hhnAqcBd2W8j9nAaWn/WIr18iq7D4oBQ8ek/SOAdcDpGe7jPcC1wFcz/Ts9AAxn/pla\nCfxl2j8SOC5jrCHgYeA5FZY5B9gKTE+vVwFvqfi6T6FY6XwGxcP7bwHPy/l9GYStiTXg7MMGI+IW\n4OdVljlGjJ0RsSHtPwZspvgfqaryIyJ+nV4ekbZKn7hKmgu8FriqynJ7SdIzKX7hXg0QEU9F3qWz\nlgA/iYhuRnyWMRWYLmkqRZL8acXl/zGwLiJ+ExF7ge9QrPNok9DEBDzWsMHKElcdJJ0EnEpRS62y\n3CFJG4FdwE0RUWn5wIeB9wH7Ky63UwDfknS7pOUZyl8A7AY+k5pSrpJ0dIY4o5YC11VZYETsAD4I\nPAjsBH4VEd+sMgZF7fdlkk6QNAN4DQcOXrAJaGICbhVJxwBfAi6NiEerLDsi9kXEiyhG9iyWdEpV\nZUs6D9gVEbdXVeY4/jTdw7nAxZLOrLj8qRTNTZ+IiFOBx4Es0xGmTv7nA1+suNzjKf4KXACcCBwt\n6Y1VxoiIzcAHgG8CNwIbgX1VxhhETUzAtQ0brJqkIyiS7zUR8eVccdKf1DcDVY6hPwM4X9IDFM1A\nZ0v6fIXlA7+r3RERu4AbKJqgqrQd2N7x18FqioScw7nAhoh4pOJyXwFsjYjdEfE08GXgpRXHICKu\njogXR8SZwC8onlvYJDQxAdc2bLBKkkTR7rg5Ij6UofyZko5L+9OBVwL3VFV+RFweEXMj4iSK78G/\nRUSltS5JR0s6dnQfeBXFn8KViYiHgYcknZwOLQHurjJGhwupuPkheRA4XdKM9HO1hOKZQqUkPTt9\nnU/R/ntt1TEGTeOGIkcPhg1Kug44CxiWtB24IiKurjIGRQ3yTcCdqZ0W4O8j4msVlT8bWJkmm54C\nrIqILF3FMpoF3FDkFKYC10bEjRnivAu4Jv1Cvx94a9UB0i+QVwJvr7rsiFgnaTWwAdgL3EGeIcNf\nknQC8DRwceaHlQPBQ5HNzGrSxCYIM7NWcAI2M6uJE7CZWU2cgM3MauIEbGZWEydgG5ekfWn2rrsk\nfTENQZ1oWWeNzpgm6fxDzWKXZid7xwRi/C9J753oNZr1mhOwHcoTEfGiiDgFeAr4r51vqtD1z1BE\nrImI9x/ilOOArhOwWdM4AVtZ/w48T9JJaS7mz1GMSpsn6VWSvi9pQ6opHwO/m7f5Hkkb6Jg5S9Jb\nJP1j2p8l6YY0b/EPJb0UeD/wR6n2/Q/pvP8m6QeSftQ5t7Gk/y7px5K+C5yMWYM0biSc9V6a4vBc\niklYABYCyyLiVknDwP8AXhERj0v6O+A9kv4f8CngbGAL8IVxiv8o8J2I+LM0au8YislwTkmT8CDp\nVSnmYop5jtekSXkepxgG/SKKn+UNQO7Jgcwq4wRshzK9Y5j0v1PMXXEisC0ibk3HTwcWAd9LQ4aP\nBL4PPJ9igpj7ANJEPWNNJ3k28GYoZm8DfpVm9+r0qrTdkV4fQ5GQjwVuiIjfpBiNmxPEBpsTsB3K\nE6O10FEpyT7eeYhiruELDzrvgM9NkoD/GxH/fFAML4tjjeY2YJusW4EzJD0PfjeD2X+gmHntJEl/\nlM67cJzPrwX+On12KK1Q8RhF7XbUN4C3dbQtz0kzc90CvE7S9DRr2n+u+N7MsnICtkmJiN3AW4Dr\nJP2I1PwQEb+laHL41/QQbtc4RVwCvFzSnRTtt4si4mcUTRp3SfqHtLrDtcD303mrgWPTkk5foFgX\n8OsUU5WaNYZnQzMzq4lrwGZmNXECNjOriROwmVlNnIDNzGriBGxmVhMnYDOzmjgBm5nV5P8DVikg\nPXomCdAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd5ef38ada0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente se cierra la sesión de _TensorFlow_ y se liberan los recursos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelado de una neurona\n",
    "\n",
    "En el modelo computacional de una neurona, las señales que viajan a través de los axones (entradas $x$) interactuan en forma de producto ($w_0x_0$) con los dentritos de otras neuronas basado en la fuerza sináptica ($w_0$) de esa sinopsis. La idea es que la fuerza sináptica (pesos $w$) se aprendan a través del control de la intensidad de influencia  de una neurona a otra. En el modelo básico, los dentritos llevan la señal al cuerpo de la célula donde se suman. Si la suma final supera un valor limite, la neurona dispara un pulso a través de su axón. En el modelo computacional, se asume, que solo la frecuencia de esta señal contiene información. Basado en este código de interpretación, se modela la frecuencia de pulsos con una función de activación. Historicamente la función de activación utilizada era la función _sigmoid_, la cual transforma un valor real a un valor entre 0 y 1.\n",
    "\n",
    "En otras palabras, cada neurona realiza un producto punto entre las entradas y los pesos, le suma el sesgo y le aplica una no-linealidad (función de activación), en este caso la función _sigmoid_ $\\sigma (x)=\\frac{1}{(1+e^{−x})}$. La forma matemática de la computación hacia adelante (_fordwar_) es similar a los clasificadores lineales ya expuestos. Las neuronas tienen la capacidad de \"gustarles\" (activación cerca a uno) o \"disgustarles\" (activación cerca de cero) ciertas regiones lineales del espacio de entrada. Entonces, con una función de perdida apropiada a la salida de la neurona, se puede convertir una unica neurona en un clasificador lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(object):\n",
    "  # ... \n",
    "  def forward(self, inputs):\n",
    "    \"\"\" assume inputs and weights are 1-D numpy arrays and bias is a number \"\"\"\n",
    "    cell_body_sum = np.sum(inputs * self.weights) + self.bias\n",
    "    firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function\n",
    "    return firing_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de activación\n",
    "\n",
    "Las funciones de activación toman un valor de entrada y le ejercen una operación matemática fija (no lineal). En la práctica, existen muchas funciones de activación distintas, a continuación se mencional las más comunes:\n",
    "\n",
    "- Sigmoid: Como se muestra en la siguiente imagen, esta función toma un valor y lo aplasta a un valor entre cero y uno, en particular, valores negativos grandes se convierten en cero y valores positivos grandes se convierten en 1. Esta función tiene dos desventajas, por un lado es propensa a saturar, en ese caso el gradiente se hace casi cero. Durante _bakpropagation_ este gradiente cercano a cero se multiplica por la salida de la neurona \"matando\" el gradiente  y el flujo de señales en la neurona sera casi nulo. Por otro lado, la salida no esta centrada en cero, esto causa que si la entrada a la neurona es siempre positiva, el gradiente durante _backpropagation_ es o siempre positivo o siempre negativo generando una dinámica muy cambiante en las actualizaciones de los pesos.\n",
    "\n",
    "<img src=\"images/sigmoidFunction.png\" width=\"70%\">\n",
    "\n",
    "- ReLU Rectified Linear Unit: Computa la función $f(x)=max(0,x)$ es decir, mientras que el valor de la entrada sea negativo, la salida vale cero, en caso contrario, el valor no se modifica. Esta función converge mas rapido al usar la optimización _stochastic gradient descent_ que las funciones _sigmoid_ y _tanh_. La implementación es más sencilla ya que no requiere funciones computacionalmente caras como exponenciales. Por otro lado, cuando existen grandes gradientes, es posible que los pesos de la neurona se actualicen de forma tal en que nunca se activa, se la considera \"muerta\". Utilizando un _learning rate_ apropiado, esto es menos frecuente. Existen muchas variantes de este tipo de funciones con otras características.\n",
    "\n",
    "<img src=\"images/reluFunction.png\" width=\"70%\">\n",
    "\n",
    "## Arquitectura de redes neuronales\n",
    "\n",
    "Las redes neuronales se modelan como colecciones de neuronas que estan conectadas en grafos no ciclicos. Es decir, las salidas de algunas neuronas son entradas de otras. En lugar de un coneccionado amorfo de las neuronas, se suele organizarlas en distintos tipos de capas. El tipo de capa más común, se denomina _fully-connected_, donde todas las neuronas entre dos capas adyacentes estan conectadas , pero no asi las neuronas de las mismas capa.\n",
    "\n",
    "<img src=\"images/neuralNetworkArch.png\" width=\"70%\">\n",
    "\n",
    "Cuando se habla de una red neuronal de N capas no se cuenta la capa de entrada, por ejemplo, una red de una capa tiene la entrada mapeada directamente a la salida, es decir no tiene capas ocultas. A diferencia de las demás capas, la de salida no suele tener funciones de activación no lineales, estos es por que la ultima capa se utiliza para representar los _scores_ de cada clase.\n",
    "\n",
    "Las métricas que usualmente se utilizan para medir el tamaño de una red neuronal son el número de neuronas y el de parámetros. Para los ejemplos de la imagen:\n",
    "\n",
    "- La de la izquierda tiene $4 + 2 = $ neuronas sin computar las de entrada. Además tiene $[3 x 4] + [4 x 2] = 20$ pesos $w$ y $4 + 2 = 6$ sesgos (_bias_) es decir 26 parámetros en total.\n",
    "- La segunda (de la derecha) tiene $4 + 4 + 1 = 9$ neuronas, $[3 x 4] + [4 x 2] = 20$ pesos y $4 + 2 = 6$ sesgos para un total de 41 parámetros.\n",
    "    \n",
    "Una de las principales razones por la que las redes neuronales se organizan en capas es que esta estructura facilita y eficientiza la evaluación de la red neuronal utilizando operaciones entre matrices y vectores. Continuando con el ejemplo de la imagen, en la red neuronal de tres capas, la entrada es un vector de la forma $[3 x 1]$. Los parámetros pueden almacenarse en una matriz para cada capa, los pesos de la primer capa oculta $W_1$ se almacenan en una matriz de $[4 x 3]$ y los sesgos en un vector $b_1$ de la forma  $[4 x 1]$.  Cada neurona tiene sus pesos en una de las filas de $w_1$, entonces la multiplicación np.dot($W_1$,$x$) evalúa la activación de todas las neuronas de esa capa. En forma similar, $w_2$ es una matriz de $[4 x 4]$ que almacena todas las conexiones de la segunda capa y $w_3$ es una matriz de $[1 x 4]$ para la ultima capa (de salida). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward-pass of a 3-layer neural network:\n",
    "f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)\n",
    "x = np.random.randn(3, 1) # random input vector of three numbers (3x1)\n",
    "h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1)\n",
    "h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4x1)\n",
    "out = np.dot(W3, h2) + b3 # output neuron (1x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el código de arriba, W1,W2,W3,b1,b2,b3 son los parámetros que se deben aprender en la red. Se debe notar, que la variable de entrada no es un vector columna, sino una matriz que contiene un sub-conjunto (_batch_) de datos de entrenamiento (donde cada entrada se almacena en una columna de x). Además, se destaca que la ultima capa no aplica una función de activación.\n",
    "\n",
    "### Arquitectura de redes neuronales en TensorFlow\n",
    "\n",
    "A continuación, se crea una función para crear la estructura de la red neuronal a partir de los hiperparámetros:\n",
    "- $num\\_dense\\_layers$: número de capas de la red.\n",
    "- $activation$: método de activación de las neuronas, puede ser ReLU o Sigmoid.\n",
    "- $learning\\_rate$:  _learning rate_ del optimizador Adam.\n",
    "- $num\\_dense\\_nodes$: número de neuronas en cada capa.\n",
    "\n",
    "La API _keras_ tiene dos modos de construcción de redes neuronales, el mas simple es el llamado _Sequential Model_ donde las capas se van agregando en forma secuencial. Como el número de capas es un hiperparámetro, se utiliza un bucle _for_ para agregar las capas secuencialmente.\n",
    "\n",
    "Una vez definida la arquitectura della red neuronal, es necesario agregar la función de pérdida, el optimizados y la métrica de medicion de _performance_. Esto se hace mediante la compilación del modelo. Como se estableció anteriormente, se elige como optimizador el algoritmo _Adam_, como funcón de pérdida $categorical\\_crossentropy$ y como métrica de _performance_ la presición de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the neural network with these hyper-parameters\n",
    "def create_model (num_dense_layers, activation,\n",
    "                 learning_rate, num_dense_nodes):\n",
    "    # Start construction of a Keras Sequential model.\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add an input layer which is similar to a feed_dict in TensorFlow.\n",
    "    # Note that the input-shape must be a tuple containing the image-size.\n",
    "    model.add(InputLayer(input_shape=(img_size_flat,)))\n",
    "\n",
    "    # Add fully-connected / dense layers.\n",
    "    # The number of layers is a hyper-parameter we want to optimize.\n",
    "    for i in range(int(num_dense_layers)):\n",
    "        # Name of the layer. This is not really necessary\n",
    "        # because Keras should give them unique names.\n",
    "        name = 'layer_dense_{0}'.format(i+1)\n",
    "\n",
    "        # Add the dense / fully-connected layer to the model.\n",
    "        # This has two hyper-parameters we want to optimize:\n",
    "        # The number of nodes and the activation function.\n",
    "        model.add(Dense(int(num_dense_nodes[i]),\n",
    "                        activation=activation,\n",
    "                        name=name))\n",
    "    # Last fully-connected / dense layer with softmax-activation\n",
    "    # for use in classification.\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Use the Adam method for training the network.\n",
    "    # We want to find the best learning-rate for the Adam method.\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "\n",
    "    # In Keras we need to compile the model so it can be trained.\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización de hiperparámetros\n",
    "\n",
    "Existen muchos hiperparámetros a elegir cuando se entrena una red neuronal en _TensorFlow_, como la cantidad de capas, la cantidad de nodos en cada capa, la función de activación, el método de optimización, el _learning-rate_ del mismo, etc. \n",
    "\n",
    "Una forma de encontrar buenos hiperparámetros es probando a mano cada uno hasta encontrar un conjunto que funcione adecuadamente. Esto lleva mucho tiempo y en algunos casos puede no ser muy intuitivo. Otra forma es determinar limites de busqueda para cada parámetro y computar todas las combinaciones (mediante la definici{on de un paso discreto). Esto se denomina _Grid Search_. Aunque es completamente computacional, rápidamente se vuelve muy lento ya que el número de combinaciones de paramétros crece exponencialmente al agregar mas parámetros (_Curse of Dimensionality_). Otro método es mediante una busqueda aleatoria, pero la probabilidad de encontrar combinaciones de parámetros optimas decrece a cero al aumentar el número de parámetros a encontrar.\n",
    "\n",
    "Finalmente, se puede utilizar Particle Swarm Optimization (PSO) para encontrar una combinación de parámetros que cumpla las condiciones necesarias. __PSO__ es un método de optimización de funciones continuas no lineales inspirado en el movimiento coordinado de animales que viven en grupos, mas específicamente en enjambres. El comportamiento (velocidad y dirección) de cada individuo es el resultado de influencias cognitivas, sociales y estocásticas. El objetivo común de todos los miembros es encontrar la mejor localización en el espacio de búsqueda.\n",
    "\n",
    "Este concepto y paradigma adhiere a 5 principios básicos de enjambres inteligentes:\n",
    "\n",
    " + Proximidad: la población debe ser capaz de realizar cálculos simples de espacio y tiempo.\n",
    " + Calidad: la población debe ser capaz de responder a factores de calidad del entorno.\n",
    " + Diversidad: la población no debe comprometer sus actividades a lo largo de canales excesivamente estrechos.\n",
    " + Estabilidad: la población no debe cambiar su comportamiento cada vez que el entorno cambia.\n",
    " + Adaptabilidad: la población se debe podres adaptar a cambios cuando el costo computacional lo permite.\n",
    "\n",
    "La forma básica del algoritmo __PSO__ esta compuesta por un grupo de partículas con comunicación entre ellas, el grupo de partículas se denomina enjambre. La posición de cada una de ellas en el espacio de búsqueda representa una posible solución al problema. Cada partícula tiene una posición, velocidad y valor de _fitness_ que se determina mediante una función de optimización. La velocidad determina la próxima dirección y distancia que se debe mover la particula. El valor de _fitness_ mide la calidad del resultado propuesto por la partícula. Cada una de ellas actualiza su valor de velocidad y posición mediante la influencia del _personal best_ (mejor solución de la particula) y el _global best_ (mejor solución de todas el enjambre).\n",
    "\n",
    " <img src=\"images/psoFlow.jpg\">\n",
    "\n",
    "Ventajas\n",
    "\n",
    " + Concepto simple se implementa en unas pocas lineas de código.\n",
    " + Requiere operadores matemáticos simples\n",
    " + No es computacionalente costoso en términos de memoria y velocidad.\n",
    " \n",
    " ### Espacio de búsqueda\n",
    " \n",
    " Los hiperparámetros a optimizar son:\n",
    " \n",
    "- $num\\_dense\\_layers$: número de capas de la red.\n",
    "- $activation$: método de activación de las neuronas, puede ser ReLU o Sigmoid.\n",
    "- $learning\\_rate$: _learning rate_ del optimizador Adam.\n",
    "- $num\\_dense\\_nodes$: número de neuronas en cada capa.\n",
    "\n",
    "A continuación se define el espacio de búsqueda definiendo los límites y una función que verifica que la partícula se encuentre dentro de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optunity PSO optimization search space\n",
    "# Activation methods\n",
    "activation_cat = ['relu', 'sigmoid']\n",
    "# Maximum and minimum number of nodes or neurons\n",
    "max_dense_nodes = 64 \n",
    "min_dense_nodes = 1\n",
    "# Maximum number of layers\n",
    "max_dense_layers = 5\n",
    "# Maximum and minimum learning rate for the optimizer\n",
    "max_learning_rate = 1e-2\n",
    "min_learning_rate = 1e-6\n",
    "\n",
    "# This function verify than the particles are inside the search space\n",
    "def verify_constrains (num_dense_layers, activation,\n",
    "                    learning_rate, num_dense_nodes):   \n",
    "    return(num_dense_layers < max_dense_layers and\n",
    "           num_dense_nodes[0] < max_dense_nodes and \n",
    "           num_dense_nodes[0] > min_dense_nodes and \n",
    "           num_dense_nodes[1] < max_dense_nodes and \n",
    "           num_dense_nodes[1] > min_dense_nodes and \n",
    "           num_dense_nodes[2] < max_dense_nodes and \n",
    "           num_dense_nodes[2] > min_dense_nodes and \n",
    "           num_dense_nodes[3] < max_dense_nodes and \n",
    "           num_dense_nodes[3] > min_dense_nodes and \n",
    "           num_dense_nodes[4] < max_dense_nodes and \n",
    "           num_dense_nodes[4] > min_dense_nodes and \n",
    "           activation < 2 and\n",
    "           learning_rate < max_learning_rate and\n",
    "           learning_rate > min_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguientes funciones se utilizan para calcular el valor _fitness_ de cada partícula, en este caso, el _fitness_ es la presición con que se logró clasificar el set de validación. Evaluar la precisión con que se clasifica, implica crear el modelo con los hiperparámetros que la partícula establece, entrenar el modelo y luego evaluarlo.\n",
    "La creación del modelo se realiza llamando a la función $create\\_model$ descripta anteriormente. Una vez que el model fue definido completamente con función de pérdida y optimizador, es necesario entrenarlo. Esta función toma dos _numpy-arrays_ y realiza tantos entrenamientos como se especifiquen en el argumento $epochs$ y utilizando el tamaño de _batch_ especificado.\n",
    "\n",
    "Finalmente, es momento obtener la métrica de calidad de clasificación (_accuracy_), esto se hace mediante la evaluación del modelo entrenado. Para esto no se puede utilizar el conjunto de _test_, por que se obtendria un set de parámetros especificamente para este set de datos causando _overfitting_. Por esto se utiliza la técnica de _k-fold cross-validation_ donde el conjunto de entrenamiento se divide en $k$ pliegues, y luego se itera realizando $k$ entrenamientos y utilizando un pligue como validaciòn distinto en cada uno de ellos. Se obtenienen 5 resultados que luego se promedian para obtener el valor final de _fitness_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the fitness value\n",
    "def calculate_fitness (num_dense_layers, activation,\n",
    "                       learning_rate, num_dense_nodes):\n",
    "    # number of folds\n",
    "    k = 5 \n",
    "    # calculate the fold size\n",
    "    n_samples = len(data.train.images)\n",
    "    fold_size = n_samples // k\n",
    "    # Create list for saving the scores \n",
    "    scores = []\n",
    "    masks = []\n",
    "    for fold in range(k):\n",
    "        # Creation of the tensorFlow model with the PSO particle parameters\n",
    "        model = create_model(num_dense_layers, activation, learning_rate, num_dense_nodes)\n",
    "        if fold == 0:\n",
    "            print(activation)\n",
    "            print(model.summary())\n",
    "        # generate a boolean mask for the test set in this fold\n",
    "        test_mask = np.zeros(n_samples, dtype=bool)\n",
    "        test_mask[fold * fold_size : (fold + 1) * fold_size] = True\n",
    "        # store the mask for visualization\n",
    "        masks.append(test_mask)\n",
    "        # create training and test sets using this mask\n",
    "        X_test, y_test = data.train.images[test_mask], data.train.labels[test_mask]\n",
    "        X_train, y_train = data.train.images[~test_mask], data.train.labels[~test_mask]\n",
    "        # fit the classifier\n",
    "        ## Training\n",
    "        model.fit(x=X_train,\n",
    "                  y=y_train,\n",
    "                  epochs=3,\n",
    "                  batch_size=128,\n",
    "                  verbose=0)\n",
    "        ## Evaluation\n",
    "        result = model.evaluate(x=X_test,\n",
    "                                y=y_test,\n",
    "                                verbose=0)\n",
    "        # compute the score and record it\n",
    "        scores.append(result[1])\n",
    "       \n",
    "        print(\"Fold: \" + str(fold))        \n",
    "        print(\"{0}: {1:.2%}\".format(model.metrics_names[1], result[1]),model.metrics_names[0],result[0])\n",
    "        # Delete the Keras model with these hyper-parameters from memory. \n",
    "        del model\n",
    "        # Clear the Keras session, otherwise it will keep adding new\n",
    "        # models to the same TensorFlow graph each time we create\n",
    "        # a model with a different set of hyper-parameters.\n",
    "        K.clear_session()\n",
    "    print(np.abs(np.mean(scores)))\n",
    "    return (np.abs(np.mean(scores)))\n",
    "\n",
    "def create_objective_function():\n",
    "    def f(num_dense_layers, activation,\n",
    "          learning_rate, dense_layer0,\n",
    "          dense_layer1,dense_layer2,\n",
    "          dense_layer3,dense_layer4):\n",
    "\n",
    "        num_dense_nodes = [dense_layer0,dense_layer1,dense_layer2,dense_layer3,dense_layer4]\n",
    "        fitness = 10\n",
    "        if verify_constrains(num_dense_layers,round(activation),learning_rate, num_dense_nodes):\n",
    "            fitness = 1.0/calculate_fitness(num_dense_layers, activation_cat[round(activation)],learning_rate, num_dense_nodes)\n",
    "        return fitness\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Optunity\n",
    " \n",
    " Optunity es una biblioteca que contiene varios optimizadores de hiperparámetros, incluyendo __PSO__. En el siguiente código, se inicializa el _solver_ con los siguientes hiperparametros del _particle swarm_:\n",
    "\n",
    "- num_particles (int): Número de partículas\n",
    "- num_generations (int): Máximo número de generaciones\n",
    "- max_speed (float or None): Velocidad máxima de cada partícula\n",
    "- phi1 (float): Coeficiente de aceleración que determina la influencia relativa del _local best_\n",
    "- phi2 (float): Coeficiente de aceleración que determina la influencia relativa del _global best_\n",
    "- kwargs ({'name': [lb, ub], ..}): Dimensiones del espacio de busqueda y sus respectivos valores máximos y mínimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 43)                33755     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 60)                2640      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                610       \n",
      "=================================================================\n",
      "Total params: 37,005\n",
      "Trainable params: 37,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 95.62% loss 0.141596140798\n",
      "Fold: 1\n",
      "acc: 95.70% loss 0.139085072947\n",
      "Fold: 2\n",
      "acc: 94.93% loss 0.162478622548\n",
      "Fold: 3\n",
      "acc: 95.72% loss 0.147750961468\n",
      "Fold: 4\n",
      "acc: 96.18% loss 0.132498652659\n",
      "0.956290909065\n",
      "relu\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 59)                46315     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 12)                720       \n",
      "_________________________________________________________________\n",
      "layer_dense_3 (Dense)        (None, 48)                624       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                490       \n",
      "=================================================================\n",
      "Total params: 48,149\n",
      "Trainable params: 48,149\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 94.88% loss 0.171122734037\n",
      "Fold: 1\n",
      "acc: 95.36% loss 0.154638556044\n",
      "Fold: 2\n",
      "acc: 94.73% loss 0.185177818885\n",
      "Fold: 3\n",
      "acc: 94.37% loss 0.182997999138\n",
      "Fold: 4\n",
      "acc: 94.95% loss 0.168592094442\n",
      "0.948599999983\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 28)                21980     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                290       \n",
      "=================================================================\n",
      "Total params: 22,270\n",
      "Trainable params: 22,270\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 93.99% loss 0.200204330878\n",
      "Fold: 1\n",
      "acc: 94.85% loss 0.182376015181\n",
      "Fold: 2\n",
      "acc: 93.95% loss 0.205962889883\n",
      "Fold: 3\n",
      "acc: 93.85% loss 0.201658117408\n",
      "Fold: 4\n",
      "acc: 94.75% loss 0.186882850976\n",
      "0.942799999983\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 12,730\n",
      "Trainable params: 12,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 90.51% loss 0.353507280241\n",
      "Fold: 1\n",
      "acc: 91.54% loss 0.331994495283\n",
      "Fold: 2\n",
      "acc: 90.38% loss 0.356271078912\n",
      "Fold: 3\n",
      "acc: 90.53% loss 0.358792843548\n",
      "Fold: 4\n",
      "acc: 91.69% loss 0.323757956109\n",
      "0.909290909065\n",
      "relu\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 31)                24335     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 16)                512       \n",
      "_________________________________________________________________\n",
      "layer_dense_3 (Dense)        (None, 5)                 85        \n",
      "_________________________________________________________________\n",
      "layer_dense_4 (Dense)        (None, 13)                78        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                140       \n",
      "=================================================================\n",
      "Total params: 25,150\n",
      "Trainable params: 25,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 93.75% loss 0.227885066526\n",
      "Fold: 1\n",
      "acc: 93.35% loss 0.246936971128\n",
      "Fold: 2\n",
      "acc: 93.57% loss 0.224629946465\n",
      "Fold: 3\n",
      "acc: 94.09% loss 0.206049926712\n",
      "Fold: 4\n",
      "acc: 94.04% loss 0.221285723559\n",
      "0.937581818173\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 63)                49455     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 48)                3072      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                490       \n",
      "=================================================================\n",
      "Total params: 53,017\n",
      "Trainable params: 53,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 95.35% loss 0.151364984537\n",
      "Fold: 1\n",
      "acc: 95.75% loss 0.144069923355\n",
      "Fold: 2\n",
      "acc: 95.76% loss 0.13878649451\n",
      "Fold: 3\n",
      "acc: 95.05% loss 0.157046471162\n",
      "Fold: 4\n",
      "acc: 96.17% loss 0.134969168159\n",
      "0.956163636346\n",
      "relu\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 8)                 6280      \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 24)                216       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                250       \n",
      "=================================================================\n",
      "Total params: 6,746\n",
      "Trainable params: 6,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 91.08% loss 0.308530451244\n",
      "Fold: 1\n",
      "acc: 92.17% loss 0.261681127483\n",
      "Fold: 2\n",
      "acc: 91.22% loss 0.304986621223\n",
      "Fold: 3\n",
      "acc: 92.50% loss 0.261682629732\n",
      "Fold: 4\n",
      "acc: 93.08% loss 0.239472101358\n",
      "0.920109090918\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 91.10% loss 0.325115968926\n",
      "Fold: 1\n",
      "acc: 92.36% loss 0.295467636639\n",
      "Fold: 2\n",
      "acc: 91.25% loss 0.311135201395\n",
      "Fold: 3\n",
      "acc: 91.07% loss 0.320062266518\n",
      "Fold: 4\n",
      "acc: 92.28% loss 0.28278618271\n",
      "0.916127272745\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 91.24% loss 0.310956580119\n",
      "Fold: 1\n",
      "acc: 92.02% loss 0.289644336782\n",
      "Fold: 2\n",
      "acc: 91.21% loss 0.310560315379\n",
      "Fold: 3\n",
      "acc: 91.10% loss 0.324618842591\n",
      "Fold: 4\n",
      "acc: 92.55% loss 0.274917202692\n",
      "0.916236363619\n",
      "relu\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 90.21% loss 0.332697768585\n",
      "Fold: 1\n",
      "acc: 92.18% loss 0.294671299783\n",
      "Fold: 2\n",
      "acc: 90.80% loss 0.328774708108\n",
      "Fold: 3\n",
      "acc: 90.05% loss 0.336273110346\n",
      "Fold: 4\n",
      "acc: 92.44% loss 0.286707208655\n",
      "0.911345454537\n",
      "relu\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 90.94% loss 0.325523182365\n",
      "Fold: 1\n",
      "acc: 92.03% loss 0.295452010225\n",
      "Fold: 2\n",
      "acc: 91.08% loss 0.312129981615\n",
      "Fold: 3\n",
      "acc: 91.39% loss 0.31083525044\n",
      "Fold: 4\n",
      "acc: 92.29% loss 0.283744875756\n",
      "0.915454545437\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 91.33% loss 0.315542024152\n",
      "Fold: 1\n",
      "acc: 92.14% loss 0.290648800975\n",
      "Fold: 2\n",
      "acc: 91.15% loss 0.319509976869\n",
      "Fold: 3\n",
      "acc: 90.97% loss 0.324664528535\n",
      "Fold: 4\n",
      "acc: 91.74% loss 0.299448633622\n",
      "0.914636363619\n",
      "relu\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 45)                35325     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 41)                1886      \n",
      "_________________________________________________________________\n",
      "layer_dense_3 (Dense)        (None, 13)                546       \n",
      "_________________________________________________________________\n",
      "layer_dense_4 (Dense)        (None, 46)                644       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                470       \n",
      "=================================================================\n",
      "Total params: 38,871\n",
      "Trainable params: 38,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 95.08% loss 0.162583817088\n",
      "Fold: 1\n",
      "acc: 96.05% loss 0.136723343914\n",
      "Fold: 2\n",
      "acc: 95.51% loss 0.156872661967\n",
      "Fold: 3\n",
      "acc: 95.16% loss 0.165797706954\n",
      "Fold: 4\n",
      "acc: 96.09% loss 0.140175416161\n",
      "0.955799999974\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 25,450\n",
      "Trainable params: 25,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 93.84% loss 0.210857979254\n",
      "Fold: 1\n",
      "acc: 94.19% loss 0.202076490467\n",
      "Fold: 2\n",
      "acc: 93.70% loss 0.217652280824\n",
      "Fold: 3\n",
      "acc: 94.04% loss 0.208106039183\n",
      "Fold: 4\n",
      "acc: 94.58% loss 0.195259432721\n",
      "0.940690909074\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 91.11% loss 0.314867698252\n",
      "Fold: 1\n",
      "acc: 92.39% loss 0.276776177661\n",
      "Fold: 2\n",
      "acc: 91.25% loss 0.308397850248\n",
      "Fold: 3\n",
      "acc: 91.27% loss 0.306538113567\n",
      "Fold: 4\n",
      "acc: 92.35% loss 0.272339848543\n",
      "0.91672727271\n",
      "relu\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 5)                 3925      \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 29)                174       \n",
      "_________________________________________________________________\n",
      "layer_dense_3 (Dense)        (None, 48)                1440      \n",
      "_________________________________________________________________\n",
      "layer_dense_4 (Dense)        (None, 28)                1372      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                290       \n",
      "=================================================================\n",
      "Total params: 7,201\n",
      "Trainable params: 7,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 89.63% loss 0.346292341146\n",
      "Fold: 1\n",
      "acc: 90.03% loss 0.321692643166\n",
      "Fold: 2\n",
      "acc: 89.39% loss 0.35649332144\n",
      "Fold: 3\n",
      "acc: 88.42% loss 0.365848339785\n",
      "Fold: 4\n",
      "acc: 88.93% loss 0.366675600328\n",
      "0.892781818164\n",
      "relu\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 13)                10205     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 63)                882       \n",
      "_________________________________________________________________\n",
      "layer_dense_3 (Dense)        (None, 45)                2880      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                460       \n",
      "=================================================================\n",
      "Total params: 14,427\n",
      "Trainable params: 14,427\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 94.04% loss 0.205992005806\n",
      "Fold: 1\n",
      "acc: 94.21% loss 0.191915391304\n",
      "Fold: 2\n",
      "acc: 93.87% loss 0.2039807627\n",
      "Fold: 3\n",
      "acc: 94.44% loss 0.189349741903\n",
      "Fold: 4\n",
      "acc: 94.39% loss 0.196948482226\n",
      "0.941890909056\n",
      "relu\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 60)                47100     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 53)                3233      \n",
      "_________________________________________________________________\n",
      "layer_dense_3 (Dense)        (None, 43)                2322      \n",
      "_________________________________________________________________\n",
      "layer_dense_4 (Dense)        (None, 38)                1672      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                390       \n",
      "=================================================================\n",
      "Total params: 54,717\n",
      "Trainable params: 54,717\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 94.78% loss 0.182066868473\n",
      "Fold: 1\n",
      "acc: 95.16% loss 0.171628289782\n",
      "Fold: 2\n",
      "acc: 94.76% loss 0.179093657089\n",
      "Fold: 3\n",
      "acc: 95.75% loss 0.166234023022\n",
      "Fold: 4\n",
      "acc: 96.18% loss 0.135340274861\n",
      "0.953272727255\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 91.27% loss 0.311734069217\n",
      "Fold: 1\n",
      "acc: 92.25% loss 0.286069939873\n",
      "Fold: 2\n",
      "acc: 91.03% loss 0.312649041333\n",
      "Fold: 3\n",
      "acc: 91.17% loss 0.309708114575\n",
      "Fold: 4\n",
      "acc: 92.46% loss 0.279158914653\n",
      "0.916363636355\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 54)                42390     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 24)                1320      \n",
      "_________________________________________________________________\n",
      "layer_dense_3 (Dense)        (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "layer_dense_4 (Dense)        (None, 46)                598       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                470       \n",
      "=================================================================\n",
      "Total params: 45,078\n",
      "Trainable params: 45,078\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 85.10% loss 0.606634635232\n",
      "Fold: 1\n",
      "acc: 83.89% loss 0.520866405444\n",
      "Fold: 2\n",
      "acc: 83.83% loss 0.671744085355\n",
      "Fold: 3\n",
      "acc: 89.82% loss 0.533000061295\n",
      "Fold: 4\n",
      "acc: 91.15% loss 0.450922931064\n",
      "0.86756363632\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 2)                 1570      \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 58)                174       \n",
      "_________________________________________________________________\n",
      "layer_dense_3 (Dense)        (None, 5)                 295       \n",
      "_________________________________________________________________\n",
      "layer_dense_4 (Dense)        (None, 9)                 54        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                100       \n",
      "=================================================================\n",
      "Total params: 2,193\n",
      "Trainable params: 2,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 53.46% loss 1.12757742942\n",
      "Fold: 1\n",
      "acc: 54.81% loss 1.20808114797\n",
      "Fold: 2\n",
      "acc: 45.05% loss 1.40035742092\n",
      "Fold: 3\n",
      "acc: 45.45% loss 1.1945322882\n",
      "Fold: 4\n",
      "acc: 35.57% loss 1.41622092568\n",
      "0.468690909087\n",
      "relu\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 29)                22765     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 33)                990       \n",
      "_________________________________________________________________\n",
      "layer_dense_3 (Dense)        (None, 10)                340       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 24,205\n",
      "Trainable params: 24,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 94.45% loss 0.192675080351\n",
      "Fold: 1\n",
      "acc: 95.04% loss 0.167492201401\n",
      "Fold: 2\n",
      "acc: 95.08% loss 0.164358507876\n",
      "Fold: 3\n",
      "acc: 94.88% loss 0.171660239033\n",
      "Fold: 4\n",
      "acc: 95.35% loss 0.16644149786\n",
      "0.949599999974\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 17)                13345     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 4)                 72        \n",
      "_________________________________________________________________\n",
      "layer_dense_3 (Dense)        (None, 28)                140       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                290       \n",
      "=================================================================\n",
      "Total params: 13,847\n",
      "Trainable params: 13,847\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 91.00% loss 0.351822508162\n",
      "Fold: 1\n",
      "acc: 91.69% loss 0.332083589445\n",
      "Fold: 2\n",
      "acc: 90.25% loss 0.397578267488\n",
      "Fold: 3\n",
      "acc: 91.22% loss 0.354182557951\n",
      "Fold: 4\n",
      "acc: 87.23% loss 0.459474276749\n",
      "0.902781818156\n",
      "relu\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 43)                33755     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                440       \n",
      "=================================================================\n",
      "Total params: 34,195\n",
      "Trainable params: 34,195\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 93.28% loss 0.236264355681\n",
      "Fold: 1\n",
      "acc: 93.89% loss 0.217901102841\n",
      "Fold: 2\n",
      "acc: 93.53% loss 0.234075574311\n",
      "Fold: 3\n",
      "acc: 93.48% loss 0.226007910197\n",
      "Fold: 4\n",
      "acc: 93.93% loss 0.219716439112\n",
      "0.936218181792\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 40)                31400     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 28)                1148      \n",
      "_________________________________________________________________\n",
      "layer_dense_3 (Dense)        (None, 35)                1015      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                360       \n",
      "=================================================================\n",
      "Total params: 33,923\n",
      "Trainable params: 33,923\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 94.26% loss 0.203178976753\n",
      "Fold: 1\n",
      "acc: 94.44% loss 0.188910945058\n",
      "Fold: 2\n",
      "acc: 94.22% loss 0.196829609074\n",
      "Fold: 3\n",
      "acc: 94.26% loss 0.193127894835\n",
      "Fold: 4\n",
      "acc: 95.11% loss 0.170911327241\n",
      "0.944581818147\n",
      "relu\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 31)                24335     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 31)                992       \n",
      "_________________________________________________________________\n",
      "layer_dense_3 (Dense)        (None, 5)                 160       \n",
      "_________________________________________________________________\n",
      "layer_dense_4 (Dense)        (None, 61)                366       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                620       \n",
      "=================================================================\n",
      "Total params: 26,473\n",
      "Trainable params: 26,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 93.91% loss 0.214498995945\n",
      "Fold: 1\n",
      "acc: 94.55% loss 0.187228388706\n",
      "Fold: 2\n",
      "acc: 94.71% loss 0.183318567376\n",
      "Fold: 3\n",
      "acc: 93.25% loss 0.235286813517\n",
      "Fold: 4\n",
      "acc: 94.43% loss 0.198678852678\n",
      "0.941672727255\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 23)                18055     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 7)                 168       \n",
      "_________________________________________________________________\n",
      "layer_dense_3 (Dense)        (None, 59)                472       \n",
      "_________________________________________________________________\n",
      "layer_dense_4 (Dense)        (None, 28)                1680      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                290       \n",
      "=================================================================\n",
      "Total params: 20,665\n",
      "Trainable params: 20,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 91.67% loss 0.324851529544\n",
      "Fold: 1\n",
      "acc: 91.73% loss 0.299893459759\n",
      "Fold: 2\n",
      "acc: 92.04% loss 0.312627408044\n",
      "Fold: 3\n",
      "acc: 91.62% loss 0.29883015042\n",
      "Fold: 4\n",
      "acc: 92.23% loss 0.295737537254\n",
      "0.918563636346\n",
      "relu\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 12)                9420      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                130       \n",
      "=================================================================\n",
      "Total params: 9,550\n",
      "Trainable params: 9,550\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 91.50% loss 0.297932428506\n",
      "Fold: 1\n",
      "acc: 92.67% loss 0.25713371687\n",
      "Fold: 2\n",
      "acc: 92.25% loss 0.265492687057\n",
      "Fold: 3\n",
      "acc: 93.12% loss 0.235574644181\n",
      "Fold: 4\n",
      "acc: 93.14% loss 0.234753573502\n",
      "0.925345454537\n",
      "relu\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 17)                13345     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                180       \n",
      "=================================================================\n",
      "Total params: 13,525\n",
      "Trainable params: 13,525\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 93.06% loss 0.244869738785\n",
      "Fold: 1\n",
      "acc: 93.85% loss 0.215987092278\n",
      "Fold: 2\n",
      "acc: 92.38% loss 0.264168457784\n",
      "Fold: 3\n",
      "acc: 92.19% loss 0.264180579034\n",
      "Fold: 4\n",
      "acc: 94.21% loss 0.210457434996\n",
      "0.931399999991\n",
      "relu\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 91.32% loss 0.316214734457\n",
      "Fold: 1\n",
      "acc: 92.39% loss 0.28531664216\n",
      "Fold: 2\n",
      "acc: 91.25% loss 0.304145953829\n",
      "Fold: 3\n",
      "acc: 91.45% loss 0.307555001855\n",
      "Fold: 4\n",
      "acc: 92.45% loss 0.275204904562\n",
      "0.917745454519\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 91.04% loss 0.326753685344\n",
      "Fold: 1\n",
      "acc: 91.95% loss 0.302397356488\n",
      "Fold: 2\n",
      "acc: 90.70% loss 0.32716101745\n",
      "Fold: 3\n",
      "acc: 90.75% loss 0.320242221735\n",
      "Fold: 4\n",
      "acc: 91.79% loss 0.29641416851\n",
      "0.912454545446\n",
      "relu\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 90.55% loss 0.336758631164\n",
      "Fold: 1\n",
      "acc: 91.80% loss 0.310178923834\n",
      "Fold: 2\n",
      "acc: 90.56% loss 0.338570332744\n",
      "Fold: 3\n",
      "acc: 90.64% loss 0.329194847519\n",
      "Fold: 4\n",
      "acc: 91.78% loss 0.303929986667\n",
      "0.910672727273\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 44)                34540     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 47)                2115      \n",
      "_________________________________________________________________\n",
      "layer_dense_3 (Dense)        (None, 17)                816       \n",
      "_________________________________________________________________\n",
      "layer_dense_4 (Dense)        (None, 57)                1026      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                580       \n",
      "=================================================================\n",
      "Total params: 39,077\n",
      "Trainable params: 39,077\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 94.29% loss 0.209568115652\n",
      "Fold: 1\n",
      "acc: 94.37% loss 0.217100314414\n",
      "Fold: 2\n",
      "acc: 94.10% loss 0.216636501562\n",
      "Fold: 3\n",
      "acc: 93.85% loss 0.233990164432\n",
      "Fold: 4\n",
      "acc: 95.15% loss 0.178843661675\n",
      "0.9435090909\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "layer_dense_3 (Dense)        (None, 22)                198       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                230       \n",
      "=================================================================\n",
      "Total params: 13,124\n",
      "Trainable params: 13,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 88.37% loss 0.456358933492\n",
      "Fold: 1\n",
      "acc: 89.51% loss 0.401335184682\n",
      "Fold: 2\n",
      "acc: 86.35% loss 0.478112260038\n",
      "Fold: 3\n",
      "acc: 88.91% loss 0.4290297544\n",
      "Fold: 4\n",
      "acc: 88.71% loss 0.481872317726\n",
      "0.8836909091\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 23)                18055     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 30)                720       \n",
      "_________________________________________________________________\n",
      "layer_dense_3 (Dense)        (None, 57)                1767      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                580       \n",
      "=================================================================\n",
      "Total params: 21,122\n",
      "Trainable params: 21,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 91.72% loss 0.307035076185\n",
      "Fold: 1\n",
      "acc: 92.03% loss 0.301279965032\n",
      "Fold: 2\n",
      "acc: 91.18% loss 0.332482447191\n",
      "Fold: 3\n",
      "acc: 90.70% loss 0.347764465516\n",
      "Fold: 4\n",
      "acc: 92.48% loss 0.279508396739\n",
      "0.916218181827\n",
      "relu\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 91.19% loss 0.307748545408\n",
      "Fold: 1\n",
      "acc: 92.08% loss 0.284577995961\n",
      "Fold: 2\n",
      "acc: 91.27% loss 0.308318014963\n",
      "Fold: 3\n",
      "acc: 91.61% loss 0.299393458767\n",
      "Fold: 4\n",
      "acc: 92.25% loss 0.279567875981\n",
      "0.916799999983\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 63)                49455     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 27)                1728      \n",
      "_________________________________________________________________\n",
      "layer_dense_3 (Dense)        (None, 32)                896       \n",
      "_________________________________________________________________\n",
      "layer_dense_4 (Dense)        (None, 54)                1782      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                550       \n",
      "=================================================================\n",
      "Total params: 54,411\n",
      "Trainable params: 54,411\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 94.04% loss 0.212935819122\n",
      "Fold: 1\n",
      "acc: 95.45% loss 0.17901816182\n",
      "Fold: 2\n",
      "acc: 95.04% loss 0.184656673524\n",
      "Fold: 3\n",
      "acc: 94.12% loss 0.202039579329\n",
      "Fold: 4\n",
      "acc: 95.13% loss 0.180120563825\n",
      "0.94752727271\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 42)                32970     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 4)                 172       \n",
      "_________________________________________________________________\n",
      "layer_dense_3 (Dense)        (None, 62)                310       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                630       \n",
      "=================================================================\n",
      "Total params: 34,082\n",
      "Trainable params: 34,082\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 93.58% loss 0.253342385812\n",
      "Fold: 1\n",
      "acc: 92.95% loss 0.292268758037\n",
      "Fold: 2\n",
      "acc: 92.29% loss 0.303167758801\n",
      "Fold: 3\n",
      "acc: 92.91% loss 0.27600588882\n",
      "Fold: 4\n",
      "acc: 94.00% loss 0.233568469776\n",
      "0.931472727281\n",
      "relu\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 90.99% loss 0.318067171411\n",
      "Fold: 1\n",
      "acc: 92.10% loss 0.292882865548\n",
      "Fold: 2\n",
      "acc: 90.85% loss 0.318160190842\n",
      "Fold: 3\n",
      "acc: 91.03% loss 0.308756757574\n",
      "Fold: 4\n",
      "acc: 92.17% loss 0.287206639488\n",
      "0.914290909082\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 36)                28260     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 22)                814       \n",
      "_________________________________________________________________\n",
      "layer_dense_3 (Dense)        (None, 50)                1150      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 30,734\n",
      "Trainable params: 30,734\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 94.65% loss 0.185852463869\n",
      "Fold: 1\n",
      "acc: 94.98% loss 0.178153798947\n",
      "Fold: 2\n",
      "acc: 94.55% loss 0.183590262424\n",
      "Fold: 3\n",
      "acc: 94.53% loss 0.194116361258\n",
      "Fold: 4\n",
      "acc: 95.26% loss 0.168377490846\n",
      "0.947927272736\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 34)                26690     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 39)                1365      \n",
      "_________________________________________________________________\n",
      "layer_dense_3 (Dense)        (None, 11)                440       \n",
      "_________________________________________________________________\n",
      "layer_dense_4 (Dense)        (None, 30)                360       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                310       \n",
      "=================================================================\n",
      "Total params: 29,165\n",
      "Trainable params: 29,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 93.36% loss 0.253860041559\n",
      "Fold: 1\n",
      "acc: 93.13% loss 0.257852955948\n",
      "Fold: 2\n",
      "acc: 93.37% loss 0.255454238529\n",
      "Fold: 3\n",
      "acc: 93.90% loss 0.228413014813\n",
      "Fold: 4\n",
      "acc: 93.01% loss 0.268037401514\n",
      "0.933545454537\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 46)                36110     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                470       \n",
      "=================================================================\n",
      "Total params: 36,580\n",
      "Trainable params: 36,580\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 94.95% loss 0.167720608478\n",
      "Fold: 1\n",
      "acc: 95.22% loss 0.163251684739\n",
      "Fold: 2\n",
      "acc: 94.57% loss 0.172726862907\n",
      "Fold: 3\n",
      "acc: 94.74% loss 0.175656656024\n",
      "Fold: 4\n",
      "acc: 95.49% loss 0.15860203141\n",
      "0.949927272719\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 37)                29045     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                380       \n",
      "=================================================================\n",
      "Total params: 29,425\n",
      "Trainable params: 29,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 94.62% loss 0.182509779895\n",
      "Fold: 1\n",
      "acc: 95.10% loss 0.163382441561\n",
      "Fold: 2\n",
      "acc: 94.82% loss 0.170021318457\n",
      "Fold: 3\n",
      "acc: 94.62% loss 0.17881562706\n",
      "Fold: 4\n",
      "acc: 95.42% loss 0.161727220754\n",
      "0.949145454545\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 91.49% loss 0.305954003264\n",
      "Fold: 1\n",
      "acc: 92.41% loss 0.28028504014\n",
      "Fold: 2\n",
      "acc: 91.44% loss 0.30870599641\n",
      "Fold: 3\n",
      "acc: 91.53% loss 0.302444114972\n",
      "Fold: 4\n",
      "acc: 92.67% loss 0.277786446479\n",
      "0.919072727255\n",
      "relu\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 33)                25905     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 11)                374       \n",
      "_________________________________________________________________\n",
      "layer_dense_3 (Dense)        (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "layer_dense_4 (Dense)        (None, 49)                588       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                500       \n",
      "=================================================================\n",
      "Total params: 27,499\n",
      "Trainable params: 27,499\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 94.18% loss 0.193546690039\n",
      "Fold: 1\n",
      "acc: 94.44% loss 0.19100908174\n",
      "Fold: 2\n",
      "acc: 93.65% loss 0.20242118027\n",
      "Fold: 3\n",
      "acc: 93.62% loss 0.212634834798\n",
      "Fold: 4\n",
      "acc: 95.00% loss 0.170185536077\n",
      "0.941781818182\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 91.07% loss 0.316565843669\n",
      "Fold: 1\n",
      "acc: 92.38% loss 0.279176503891\n",
      "Fold: 2\n",
      "acc: 91.38% loss 0.306700700083\n",
      "Fold: 3\n",
      "acc: 91.44% loss 0.301044252141\n",
      "Fold: 4\n",
      "acc: 92.25% loss 0.2813225052\n",
      "0.917036363645\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 27)                21195     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                280       \n",
      "=================================================================\n",
      "Total params: 21,475\n",
      "Trainable params: 21,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 94.61% loss 0.185034474302\n",
      "Fold: 1\n",
      "acc: 94.35% loss 0.190698594548\n",
      "Fold: 2\n",
      "acc: 94.09% loss 0.197407505577\n",
      "Fold: 3\n",
      "acc: 94.10% loss 0.195355201206\n",
      "Fold: 4\n",
      "acc: 94.97% loss 0.177046111116\n",
      "0.944254545446\n",
      "relu\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 91.10% loss 0.312565153111\n",
      "Fold: 1\n",
      "acc: 92.11% loss 0.287714742108\n",
      "Fold: 2\n",
      "acc: 91.12% loss 0.316934207201\n",
      "Fold: 3\n",
      "acc: 91.00% loss 0.311317188176\n",
      "Fold: 4\n",
      "acc: 92.40% loss 0.279378587945\n",
      "0.915454545455\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 20)                15700     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 15,910\n",
      "Trainable params: 15,910\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 93.43% loss 0.222386621475\n",
      "Fold: 1\n",
      "acc: 93.70% loss 0.224513486911\n",
      "Fold: 2\n",
      "acc: 93.26% loss 0.233540205739\n",
      "Fold: 3\n",
      "acc: 92.97% loss 0.234276551518\n",
      "Fold: 4\n",
      "acc: 93.70% loss 0.219381869327\n",
      "0.934127272719\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 91.11% loss 0.309912343399\n",
      "Fold: 1\n",
      "acc: 92.02% loss 0.289852830957\n",
      "Fold: 2\n",
      "acc: 91.39% loss 0.304246735649\n",
      "Fold: 3\n",
      "acc: 91.55% loss 0.305626267076\n",
      "Fold: 4\n",
      "acc: 92.23% loss 0.28065060592\n",
      "0.916581818156\n",
      "sigmoid\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fold: 0\n",
      "acc: 95.17% loss 0.158030968671\n",
      "Fold: 1\n",
      "acc: 95.74% loss 0.139401185404\n",
      "Fold: 2\n",
      "acc: 95.35% loss 0.154377014423\n",
      "Fold: 3\n",
      "acc: 95.07% loss 0.165098195767\n",
      "Fold: 4\n",
      "acc: 95.27% loss 0.161116322068\n",
      "0.953218181784\n",
      "{'num_dense_layers': 2.65234375, 'activation': 0.681640625, 'learning_rate': 0.0085841259765625, 'dense_layer0': 43.7587890625, 'dense_layer1': 60.1240234375, 'dense_layer2': 32.8076171875, 'dense_layer3': 48.9267578125, 'dense_layer4': 44.2509765625}\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "f = create_objective_function()\n",
    "omega = 1.0/(2.0*np.log(2))\n",
    "phi = 0.5 + np.log(2)\n",
    "pso = optunity.solvers.ParticleSwarm(num_particles=10,\n",
    "                                     num_generations=500,\n",
    "                                     max_speed=omega, \n",
    "                                     phi1=phi,phi2=phi,\n",
    "                                     num_dense_layers=[1, max_dense_layers], \n",
    "                                     activation=[0, 2], \n",
    "                                     learning_rate=[min_learning_rate, max_learning_rate], \n",
    "                                     dense_layer0=[1, max_dense_nodes], \n",
    "                                     dense_layer1=[1, max_dense_nodes], \n",
    "                                     dense_layer2=[1, max_dense_nodes], \n",
    "                                     dense_layer3=[1, max_dense_nodes], \n",
    "                                     dense_layer4=[1, max_dense_nodes])\n",
    "pars,details = pso.minimize(f)\n",
    "end_time = time.time()\n",
    "time_dif = end_time - start_time\n",
    "print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1162.0805633068085\n",
      "2 sigmoid 0.0085841259765625 [43, 60, 32, 48, 44]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "layer_dense_1 (Dense)        (None, 43)                33755     \n",
      "_________________________________________________________________\n",
      "layer_dense_2 (Dense)        (None, 60)                2640      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                610       \n",
      "=================================================================\n",
      "Total params: 37,005\n",
      "Trainable params: 37,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "acc: 95.92% loss 0.133506590831\n"
     ]
    }
   ],
   "source": [
    "print(\"Time: \" + str(time_dif))\n",
    "\n",
    "num_dense_nodes = [int(pars['dense_layer0']),\n",
    "                   int(pars['dense_layer1']),\n",
    "                   int(pars['dense_layer2']),\n",
    "                   int(pars['dense_layer3']),\n",
    "                   int(pars['dense_layer4'])]\n",
    "\n",
    "num_dense_layers = (int(pars['num_dense_layers']))\n",
    "activation = activation_cat[round(pars['activation'])]\n",
    "learning_rate = pars['learning_rate']\n",
    "\n",
    "print(num_dense_layers,activation,learning_rate,num_dense_nodes)\n",
    "model = create_model(num_dense_layers, activation, learning_rate, num_dense_nodes)\n",
    "print(model.summary())\n",
    "model.fit(x=data.train.images,\n",
    "          y=data.train.labels,\n",
    "          epochs=3,\n",
    "          batch_size=128,\n",
    "          verbose=0)\n",
    "## Evaluation\n",
    "result = model.evaluate(x=data.test.images,\n",
    "                        y=data.test.labels,\n",
    "                        verbose=0)\n",
    "\n",
    "print(\"{0}: {1:.2%}\".format(model.metrics_names[1], result[1]),model.metrics_names[0],result[0])\n",
    "# Delete the Keras model with these hyper-parameters from memory. \n",
    "del model\n",
    "# Clear the Keras session, otherwise it will keep adding new\n",
    "# models to the same TensorFlow graph each time we create\n",
    "# a model with a different set of hyper-parameters.\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronales convolucionales\n",
    "\n",
    "El clasificador lineal que se mostro anteriomente, logro una presicion de aproximadamente el 91%, a continuaciòn se implementara una red nuronal convolucional simple con una presicion de aproximadamente el 99%.\n",
    "\n",
    "Las redes convolucionales trabajan moviendo filtros de pocos pixeles a través de la imagen de entrada. Esto significa que el filtro se reutiliza para reconocer patrones en toda la imagen de entrada. Esto hace que las redes neuronales convolucionales sean mas poderosas que las redes _Fully-connected_ del mismo numero de variables. Incluso las redes neuronales convolucionales son mas rapidas de entrenar.\n",
    "\n",
    "### Diagrama de flujo\n",
    "\n",
    "The following chart shows roughly how the data flows in the Convolutional Neural Network that is implemented below.\n",
    "El siguiente diagrama muestra como se mueven los datos en la red neuronal convolucional que se implementará a continuación.\n",
    "\n",
    "<img src=\"images/network_flowchart.png\" width=\"70%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La imagen de entrada es procesada en la primer capa convolucional utilizando el filtro de $5x5$ pixeles. Esto resulta en 16 nuevas imágenes, una para cada filtro de la capa. Las imagenes son sub-sampleadas por lo que la resolucion se reduce de $28x28$ a $14x14$.\n",
    "\n",
    "Estas 16 imagágenes mas chicas, son procesadas luego por la segunda capa convolucional. Se necesitan filtros por cada uno de los 16 canales y, además, se necesitan filtros para cada canal de salida de esta capa. Hay 36 canales de salida por lo que  en total hay $16 x 36 = 576$ filtros. Las imagenes resultantes estan sub-sampleadas nuevamente a $7 x 7$ pixeles.\n",
    "\n",
    "La salida de la segunda capa convolucional es de 36 imágenes de $7x7$ pixeles. Luego se le cambia la forma a un vector de largo $7x7x36 = 1764$ y de una dimensión, el cual se usa como entrada a una capa _fully-connected_ de 128 neuronas. Esto alimenta otra capa _fully-connected_ de 10 neuronas, una por cada clase, que se usan para determinar a que clase se corresponde la imagne, es decir, que numero esta dibujado en la imagen.\n",
    "\n",
    "Los filtros convolucionales son inicializados con valores aleatorio, por lo que la clasificación es aleatoria. El error entre la clase predecida y la clase verdadera de la imagen es medida mediante la entropia cruzada. El optimizador, luego propaga este error hacia atras a traves de la red convolucional usando la _chain-rule_ de la diferenciacion y actualiza los pesos (filtros) para disminuir el error de clasificación. Esto se hace iterativamente miles de veces hasta que el error de clasificación es lo suficientemente chico.\n",
    "\n",
    "Se debe destacar que la computación en _TensorFlow_ se realiza en sub-conjuntos (_batches_) de imagenes, lo que implica que el diagrama de flujo en realidad tiene una dimensión de datos más cuando se implementa en _TensorFlow_.\n",
    "\n",
    "\n",
    "Convolutional Layer\n",
    "\n",
    "The following chart shows the basic idea of processing an image in the first convolutional layer. The input image depicts the number 7 and four copies of the image are shown here, so we can see more clearly how the filter is being moved to different positions of the image. For each position of the filter, the dot-product is being calculated between the filter and the image pixels under the filter, which results in a single pixel in the output image. So moving the filter across the entire input image results in a new image being generated.\n",
    "\n",
    "The red filter-weights means that the filter has a positive reaction to black pixels in the input image, while blue pixels means the filter has a negative reaction to black pixels.\n",
    "\n",
    "In this case it appears that the filter recognizes the horizontal line of the 7-digit, as can be seen from its stronger reaction to that line in the output image.\n",
    "\n",
    "Convolution example\n",
    "\n",
    "\n",
    "<img src=\"images/convolution.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#Importante: En el PSO tengo que usar si o si cross validation! Ver la pagina de Optunity \n",
    "\n",
    "Cada nodo en el grapho representa una operación matemática, mientras que los bordes del grafico representan vectores de datos (tensores) que se comunican a traves de los nodos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
